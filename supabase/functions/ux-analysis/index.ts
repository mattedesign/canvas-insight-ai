import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.52.1';

/**
 * Smart Text Formatter for Converting JSON Analysis to Human-Readable Text
 */
class SmartTextFormatter {
  static formatAnalysisDescription(data: any): string {
    if (!data || typeof data !== 'object') {
      return 'Analysis data not available';
    }

    // Handle different data structures
    if (typeof data === 'string') {
      try {
        data = JSON.parse(data);
      } catch {
        return data;
      }
    }

    // Extract key information
    const domain = data.domain || data.detectedDomain || 'general';
    const screenType = data.screen_type || data.screenType || data.interfaceType || 'interface';
    const findings = data.findings || data.analysis || data;

    let description = '';

    // Start with domain and screen type context
    description += this.formatContextIntro(domain, screenType);

    // Add main findings
    if (findings) {
      description += this.formatFindings(findings, domain);
    }

    // Add suggestions if available
    if (data.suggestions && Array.isArray(data.suggestions)) {
      description += this.formatSuggestions(data.suggestions);
    }

    return description.trim();
  }

  private static formatContextIntro(domain: string, screenType: string): string {
    const domainLabels: Record<string, string> = {
      ecommerce: 'e-commerce',
      finance: 'financial',
      healthcare: 'healthcare',
      education: 'educational',
      saas: 'SaaS',
      portfolio: 'portfolio',
      blog: 'blog',
      general: 'web'
    };

    const screenLabels: Record<string, string> = {
      landing_page: 'landing page',
      product_page: 'product page',
      checkout: 'checkout flow',
      dashboard: 'dashboard',
      form: 'form interface',
      navigation: 'navigation',
      mobile_app: 'mobile application',
      desktop_app: 'desktop application'
    };

    const domainLabel = domainLabels[domain.toLowerCase()] || domain;
    const screenLabel = screenLabels[screenType.toLowerCase()] || screenType;

    return `This ${domainLabel} ${screenLabel} analysis reveals several key insights. `;
  }

  private static formatFindings(findings: any, domain: string): string {
    let result = '';

    // Handle different finding structures
    if (typeof findings === 'string') {
      return findings + ' ';
    }

    // E-commerce specific formatting
    if (domain.toLowerCase().includes('ecommerce') || domain.toLowerCase().includes('commerce')) {
      result += this.formatEcommerceFindings(findings);
    }
    // Finance specific formatting
    else if (domain.toLowerCase().includes('finance') || domain.toLowerCase().includes('banking')) {
      result += this.formatFinanceFindings(findings);
    }
    // General formatting
    else {
      result += this.formatGeneralFindings(findings);
    }

    return result;
  }

  private static formatEcommerceFindings(findings: any): string {
    let result = '';

    // Common e-commerce patterns
    if (findings.cart_optimization || findings.checkout_optimization || findings.cart_and_checkout_flow_optimization) {
      result += 'The shopping experience shows opportunities for cart and checkout optimization. ';
    }

    if (findings.product_presentation || findings.product_display) {
      result += 'Product presentation and display elements could be enhanced for better user engagement. ';
    }

    if (findings.trust_signals || findings.security_indicators) {
      result += 'Trust signals and security indicators are important for customer confidence. ';
    }

    if (findings.mobile_experience || findings.responsive_design) {
      result += 'Mobile shopping experience requires attention for optimal conversion rates. ';
    }

    // Extract specific insights from the findings object
    const insights = this.extractInsights(findings);
    if (insights.length > 0) {
      result += insights.join(' ') + ' ';
    }

    return result;
  }

  private static formatFinanceFindings(findings: any): string {
    let result = '';

    if (findings.security || findings.compliance) {
      result += 'Security and compliance considerations are critical for financial interfaces. ';
    }

    if (findings.data_visualization || findings.dashboard_design) {
      result += 'Data visualization and dashboard design impact user decision-making. ';
    }

    const insights = this.extractInsights(findings);
    if (insights.length > 0) {
      result += insights.join(' ') + ' ';
    }

    return result;
  }

  private static formatGeneralFindings(findings: any): string {
    let result = '';

    // Common UX patterns
    if (findings.usability || findings.user_experience) {
      result += 'Usability and user experience elements show potential for improvement. ';
    }

    if (findings.accessibility || findings.wcag_compliance) {
      result += 'Accessibility compliance and inclusive design practices should be considered. ';
    }

    if (findings.visual_hierarchy || findings.design_consistency) {
      result += 'Visual hierarchy and design consistency affect user comprehension. ';
    }

    if (findings.performance || findings.loading_experience) {
      result += 'Performance and loading experience impact user satisfaction. ';
    }

    const insights = this.extractInsights(findings);
    if (insights.length > 0) {
      result += insights.join(' ') + ' ';
    }

    return result;
  }

  private static extractInsights(findings: any): string[] {
    const insights: string[] = [];

    // Look for specific patterns in the data
    Object.entries(findings).forEach(([key, value]) => {
      if (typeof value === 'string' && value.length > 10 && value.length < 200) {
        // Clean up the insight text
        const insight = this.cleanInsightText(value);
        if (insight && !insight.includes('{') && !insight.includes('[')) {
          insights.push(insight);
        }
      } else if (typeof value === 'object' && value && !Array.isArray(value)) {
        // Recursively extract from nested objects
        const nestedInsights = this.extractInsights(value);
        insights.push(...nestedInsights);
      }
    });

    return insights.slice(0, 3); // Limit to 3 insights to avoid overwhelming
  }

  private static cleanInsightText(text: string): string {
    // Remove common technical prefixes/suffixes
    let cleaned = text.replace(/^(finding|insight|recommendation|suggestion):\s*/i, '');
    cleaned = cleaned.replace(/\s*(\.|\,)$/, '');
    
    // Ensure proper capitalization
    cleaned = cleaned.charAt(0).toUpperCase() + cleaned.slice(1);
    
    // Ensure it ends with proper punctuation
    if (!/[.!?]$/.test(cleaned)) {
      cleaned += '.';
    }

    return cleaned;
  }

  private static formatSuggestions(suggestions: any[]): string {
    if (!suggestions || !Array.isArray(suggestions) || suggestions.length === 0) {
      return '';
    }

    let result = 'Key recommendations include: ';
    
    const formattedSuggestions = suggestions
      .slice(0, 3) // Limit to top 3 suggestions
      .map(suggestion => {
        if (typeof suggestion === 'string') {
          return this.cleanInsightText(suggestion);
        } else if (suggestion && suggestion.text) {
          return this.cleanInsightText(suggestion.text);
        } else if (suggestion && suggestion.description) {
          return this.cleanInsightText(suggestion.description);
        }
        return null;
      })
      .filter(Boolean);

    if (formattedSuggestions.length > 0) {
      result += formattedSuggestions.join(', ') + '.';
    } else {
      result = ''; // Remove the intro if no valid suggestions
    }

    return result;
  }

  static formatFallback(data: any): string {
    if (!data) {
      return 'Analysis data is not available at this time.';
    }

    if (typeof data === 'string') {
      // If it's already a string, try to clean it up
      if (data.includes('{') || data.includes('[')) {
        return 'Analysis completed with technical findings available for review.';
      }
      return data;
    }

    return 'Analysis completed. Please review the detailed findings in the analysis panel.';
  }
}

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

const supabaseUrl = Deno.env.get('SUPABASE_URL')!
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
const supabase = createClient(supabaseUrl, supabaseServiceKey)

// Analysis validation utility for data normalization
interface ValidationResult {
  isValid: boolean;
  warnings: string[];
  data: any;
}

class AnalysisValidator {
  static validateAndNormalize(rawAnalysis: any): ValidationResult {
    const warnings: string[] = [];
    
    if (!rawAnalysis || typeof rawAnalysis !== 'object') {
      warnings.push('Analysis data is null or not an object');
      return {
        isValid: false,
        warnings,
        data: this.createDefaultAnalysis()
      };
    }

    const suggestions = this.validateSuggestions(rawAnalysis.suggestions, warnings);
    const visualAnnotations = this.validateAnnotations(rawAnalysis.visual_annotations || rawAnalysis.visualAnnotations, warnings);
    const summary = this.validateSummary(rawAnalysis.summary, warnings);
    const metadata = this.validateMetadata(rawAnalysis.metadata, warnings);

    const normalizedAnalysis = {
      id: rawAnalysis.id || `analysis_${Date.now()}`,
      imageId: rawAnalysis.image_id || rawAnalysis.imageId || '',
      imageName: rawAnalysis.image_name || rawAnalysis.imageName || 'Untitled Image',
      imageUrl: rawAnalysis.image_url || rawAnalysis.imageUrl || '',
      userContext: rawAnalysis.user_context || rawAnalysis.userContext || '',
      visualAnnotations,
      suggestions,
      summary,
      metadata,
      createdAt: new Date(rawAnalysis.created_at || rawAnalysis.createdAt || Date.now()),
      modelUsed: rawAnalysis.model_used || rawAnalysis.modelUsed || 'unknown',
      status: rawAnalysis.status || 'completed'
    };

    return {
      isValid: warnings.length === 0,
      warnings,
      data: normalizedAnalysis
    };
  }

  private static validateSuggestions(rawSuggestions: any, warnings: string[]): any[] {
    if (!Array.isArray(rawSuggestions)) {
      warnings.push('Suggestions is not an array, using empty array');
      return [];
    }

    return rawSuggestions.map((suggestion, index) => {
      if (!suggestion || typeof suggestion !== 'object') {
        warnings.push(`Suggestion at index ${index} is invalid`);
        return this.createDefaultSuggestion(index);
      }

      return {
        id: suggestion.id || `suggestion_${index}`,
        category: this.validateSuggestionCategory(suggestion.category),
        title: suggestion.title || 'Untitled Suggestion',
        description: suggestion.description || 'No description provided',
        impact: suggestion.impact || 'medium',
        effort: suggestion.effort || 'medium',
        actionItems: Array.isArray(suggestion.actionItems) ? suggestion.actionItems : 
                    Array.isArray(suggestion.action_items) ? suggestion.action_items : [],
        relatedAnnotations: Array.isArray(suggestion.relatedAnnotations) ? suggestion.relatedAnnotations : []
      };
    });
  }

  private static validateSuggestionCategory(category: any): string {
    const validCategories = ['usability', 'accessibility', 'visual', 'content', 'performance'];
    return validCategories.includes(category) ? category : 'usability';
  }

  private static validateAnnotations(rawAnnotations: any, warnings: string[]): any[] {
    if (!Array.isArray(rawAnnotations)) {
      warnings.push('Visual annotations is not an array, using empty array');
      return [];
    }

    return rawAnnotations.map((annotation, index) => {
      if (!annotation || typeof annotation !== 'object') {
        warnings.push(`Annotation at index ${index} is invalid`);
        return this.createDefaultAnnotation(index);
      }

      return {
        id: annotation.id || `annotation_${index}`,
        x: typeof annotation.x === 'number' ? annotation.x : 0,
        y: typeof annotation.y === 'number' ? annotation.y : 0,
        type: annotation.type || 'issue',
        title: annotation.title || 'Untitled Issue',
        description: annotation.description || 'No description provided',
        severity: annotation.severity || 'medium'
      };
    });
  }

  private static validateSummary(rawSummary: any, warnings: string[]): any {
    if (!rawSummary || typeof rawSummary !== 'object') {
      warnings.push('Summary is missing or invalid, using default values');
      return this.createDefaultSummary();
    }

    return {
      overallScore: typeof rawSummary.overallScore === 'number' ? rawSummary.overallScore : 
                   typeof rawSummary.overall_score === 'number' ? rawSummary.overall_score : 75,
      categoryScores: this.validateCategoryScores(rawSummary.categoryScores || rawSummary.category_scores),
      keyIssues: Array.isArray(rawSummary.keyIssues) ? rawSummary.keyIssues :
                Array.isArray(rawSummary.key_issues) ? rawSummary.key_issues : [],
      strengths: Array.isArray(rawSummary.strengths) ? rawSummary.strengths : []
    };
  }

  private static validateCategoryScores(scores: any): any {
    const defaultScores = { usability: 50, accessibility: 50, visual: 50, content: 50 };
    
    if (!scores || typeof scores !== 'object') {
      return defaultScores;
    }

    return {
      usability: typeof scores.usability === 'number' ? scores.usability : defaultScores.usability,
      accessibility: typeof scores.accessibility === 'number' ? scores.accessibility : defaultScores.accessibility,
      visual: typeof scores.visual === 'number' ? scores.visual : defaultScores.visual,
      content: typeof scores.content === 'number' ? scores.content : defaultScores.content
    };
  }

  private static validateMetadata(rawMetadata: any, warnings: string[]): any {
    if (!rawMetadata || typeof rawMetadata !== 'object') {
      warnings.push('Metadata is missing, using default vision metadata');
      return this.createDefaultMetadata();
    }

    return {
      objects: Array.isArray(rawMetadata.objects) ? rawMetadata.objects : [],
      text: Array.isArray(rawMetadata.text) ? rawMetadata.text : [],
      colors: Array.isArray(rawMetadata.colors) ? rawMetadata.colors : [],
      faces: typeof rawMetadata.faces === 'number' ? rawMetadata.faces : 0
    };
  }

  private static createDefaultAnalysis(): any {
    return {
      id: `analysis_${Date.now()}`,
      imageId: '',
      imageName: 'Untitled Image',
      imageUrl: '',
      userContext: '',
      visualAnnotations: [],
      suggestions: [],
      summary: this.createDefaultSummary(),
      metadata: this.createDefaultMetadata(),
      createdAt: new Date(),
      modelUsed: 'fallback',
      status: 'completed'
    };
  }

  private static createDefaultSuggestion(index: number = 0): any {
    return {
      id: `suggestion_${index}`,
      category: 'usability',
      title: 'Analysis Incomplete',
      description: 'Unable to generate specific suggestion due to data issues',
      impact: 'low',
      effort: 'low',
      actionItems: [],
      relatedAnnotations: []
    };
  }

  private static createDefaultAnnotation(index: number = 0): any {
    return {
      id: `annotation_${index}`,
      x: 0,
      y: 0,
      type: 'issue',
      title: 'Data Issue',
      description: 'Invalid annotation data',
      severity: 'low'
    };
  }

  private static createDefaultSummary(): any {
    return {
      overallScore: 50,
      categoryScores: {
        usability: 50,
        accessibility: 50,
        visual: 50,
        content: 50
      },
      keyIssues: ['Unable to analyze due to data issues'],
      strengths: []
    };
  }

  private static createDefaultMetadata(): any {
    return {
      objects: [],
      text: [],
      colors: [],
      faces: 0
    };
  }
}

// Available model configurations
const MODEL_CONFIGS = {
  'gpt-4o': {
    api: 'openai',
    endpoint: 'https://api.openai.com/v1/chat/completions',
    requiresKey: 'OPENAI_API_KEY'
  },
  'claude-opus-4-20250514': {
    api: 'anthropic',
    endpoint: 'https://api.anthropic.com/v1/messages',
    requiresKey: 'ANTHROPIC_API_KEY'
  },
  'perplexity-research': {
    api: 'perplexity',
    endpoint: 'https://api.perplexity.ai/chat/completions',
    model: 'llama-3.1-sonar-small-128k-online',
    requiresKey: 'PERPLEXITY_API_KEY'
  },
  'stability-inpainting': {
    api: 'stability',
    endpoint: 'https://api.stability.ai/v1/generation/stable-diffusion-v1-6/image-to-image',
    requiresKey: 'STABILITY_API_KEY'
  }
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    const body = await req.json()
    console.log('üî• Pipeline Request:', {
      stage: body.stage,
      model: body.model,
      timestamp: new Date().toISOString()
    })
    
    // Check available API keys
    const availableAPIs = {
      openai: !!Deno.env.get('OPENAI_API_KEY'),
      anthropic: !!Deno.env.get('ANTHROPIC_API_KEY'),
      google: !!Deno.env.get('GOOGLE_VISION_API_KEY'),
      perplexity: !!Deno.env.get('PERPLEXITY_API_KEY'),
      stability: !!Deno.env.get('STABILITY_API_KEY')
    }
    
    console.log('üìù Available APIs:', availableAPIs)

    // Handle special actions first
    if (body.action) {
      switch (body.action) {
        case 'store':
          return await storeAnalysisResults(body.pipelineResults)
        
        case 'check-keys':
          return new Response(
            JSON.stringify({ available: availableAPIs }),
            { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
          )
        
        case 'ANALYZE_IMAGE':
          // Convert old Canvas format to new pipeline format
          return await handleCanvasRequest(body.action, body)
        
        case 'get-parsing-metrics':
          // PHASE 4.2: Return JSON parsing metrics for monitoring
          return new Response(
            JSON.stringify(getParsingMetrics()),
            { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
          )
        
        case 'ENHANCED_CONTEXT_ANALYSIS':
          // Handle enhanced context-aware analysis from EnhancedAnalysisPipeline
          console.log('üöÄ Processing ENHANCED_CONTEXT_ANALYSIS request')
          return await handleEnhancedContextAnalysis(body)
        
        default:
          throw new Error(`Unknown action: ${body.action}`)
      }
    }

    // Handle pipeline requests (must have stage and model)
    if (!body.stage) {
      throw new Error(`Stage is required for pipeline execution`)
    }
    
    if (!body.model) {
      throw new Error(`Model is required for pipeline execution`)
    }

    return await executeModel(body)

  } catch (error) {
    console.error('‚ùå Edge function error:', error)
    return new Response(
      JSON.stringify({ 
        error: error.message,
        details: error.stack,
        timestamp: new Date().toISOString()
      }),
      { 
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )
  }
})

async function executeModel(payload: any) {
  console.log('executeModel - Full payload received:', JSON.stringify(payload, null, 2))
  
  const { model, stage, imageUrl, imageBase64, prompt, systemPrompt, visionData, analysisData } = payload
  
  console.log('executeModel - Extracted fields:', { 
    model, 
    stage, 
    hasImageUrl: !!imageUrl, 
    hasImageBase64: !!imageBase64,
    hasPrompt: !!prompt, 
    hasSystemPrompt: !!systemPrompt,
    hasVisionData: !!visionData,
    hasAnalysisData: !!analysisData
  })
  
  if (!model) {
    throw new Error(`Model is required but received: ${model}. Full payload: ${JSON.stringify(payload)}`)
  }
  
  if (!stage) {
    throw new Error(`Stage is required but received: ${stage}. Full payload: ${JSON.stringify(payload)}`)
  }
  
  // Map newer model names to edge function model names
  const mappedModel = mapModelName(model)
  const modelConfig = MODEL_CONFIGS[mappedModel]
  
  if (!modelConfig) {
    const availableModels = Object.keys(MODEL_CONFIGS)
    const suggestion = findBestModelMatch(model, availableModels)
    console.error(`Model mapping failed:`, {
      requestedModel: model,
      mappedModel,
      availableModels,
      suggestion
    })
    throw new Error(`Unknown model: ${model} (mapped to: ${mappedModel}). Available models: ${availableModels.join(', ')}. Did you mean: ${suggestion}?`)
  }

  const apiKey = Deno.env.get(modelConfig.requiresKey)
  if (!apiKey) {
    throw new Error(
      `${modelConfig.requiresKey} not configured. This model requires ${modelConfig.api} API access.`
    )
  }

  // Execute based on API type
  switch (modelConfig.api) {
    case 'openai':
      return await executeOpenAI(modelConfig, apiKey, payload)
    
    case 'anthropic':
      return await executeAnthropic(modelConfig, apiKey, payload)
    
    case 'perplexity':
      return await executePerplexity(modelConfig, apiKey, payload)
    
    case 'stability':
      return await executeStability(modelConfig, apiKey, payload)
    
    default:
      throw new Error(`Unsupported API: ${modelConfig.api}`)
  }
}

// Helper function to ensure prompts contain "json" for OpenAI compatibility
function ensureJsonInPrompt(prompt: string): string {
  if (!prompt.toLowerCase().includes('json')) {
    console.log('‚ö†Ô∏è Adding JSON requirement to prompt for OpenAI compatibility');
    return prompt + '\n\nPlease format your response as JSON.';
  }
  return prompt;
}

// PHASE 4.2: JSON Parsing Monitoring
interface JsonParsingMetrics {
  model: string;
  stage: string;
  method: 'direct' | 'smart-pattern' | 'content-cleaning' | 'failed';
  success: boolean;
  responseLength: number;
  parseTime: number;
}

// Simple in-memory metrics store for edge function
const parsingMetrics: JsonParsingMetrics[] = [];

function trackJsonParsing(
  model: string,
  stage: string,
  method: 'direct' | 'smart-pattern' | 'content-cleaning' | 'failed',
  success: boolean,
  responseLength: number,
  parseTime: number
) {
  const metric: JsonParsingMetrics = {
    model,
    stage,
    method,
    success,
    responseLength,
    parseTime
  };
  
  parsingMetrics.push(metric);
  
  // Keep only recent metrics to avoid memory bloat
  if (parsingMetrics.length > 500) {
    parsingMetrics.splice(0, 250); // Remove oldest half
  }
  
  console.log(`üìä JSON Parse Tracking [${model}/${stage}]: ${method} ${success ? '‚úÖ' : '‚ùå'} (${parseTime}ms)`);
}

// PHASE 1: Enhanced Smart Response Parsing with fallback and validation
function parseJsonFromResponse(responseText: string, model: string, stage: string): { success: boolean; data?: any; error?: string } {
  console.log('üîç PHASE 1: Enhanced JSON parsing with validation');
  const startTime = Date.now();
  
  // Quick validation check
  if (!responseText || typeof responseText !== 'string') {
    trackJsonParsing(model, stage, 'failed', false, 0, Date.now() - startTime);
    return { success: false, error: 'Empty or invalid response text' };
  }
  
  try {
    // Method 1: Try direct JSON parsing first
    const directParse = JSON.parse(responseText);
    const parseTime = Date.now() - startTime;
    trackJsonParsing(model, stage, 'direct', true, responseText.length, parseTime);
    console.log('‚úÖ Direct JSON parse successful');
    
    // Validate the parsed data has required structure
    const validationResult = validateAnalysisStructure(directParse);
    if (validationResult.isValid) {
      return { success: true, data: directParse };
    } else {
      console.log('‚ö†Ô∏è Direct parse succeeded but validation failed:', validationResult.errors);
      // Continue to extraction methods for better results
    }
  } catch (directError) {
    console.log('‚ö†Ô∏è Direct JSON parse failed, trying extraction methods');
  }
  
  // Method 2: Enhanced regex patterns for JSON extraction
  const jsonPatterns = [
    /\{[\s\S]*?\}/g,                                    // Multiple JSON objects
    /```json\s*(\{[\s\S]*?\})\s*```/gi,                 // JSON in code blocks (case insensitive)
    /```\s*(\{[\s\S]*?\})\s*```/g,                      // JSON in plain code blocks
    /(?:^|\n)\s*(\{[\s\S]*?\})\s*(?:\n|$)/g,           // JSON at start/end of lines
    /"?analysis"?\s*:\s*(\{[\s\S]*?\})/gi,             // Analysis field extraction
    /"?result"?\s*:\s*(\{[\s\S]*?\})/gi,               // Result field extraction
    /(?:Here's|Here is)[\s\S]*?(\{[\s\S]*?\})/gi       // Common AI response patterns
  ];
  
  for (const pattern of jsonPatterns) {
    let match;
    while ((match = pattern.exec(responseText)) !== null) {
      const extractedJson = match[1] || match[0];
      try {
        const parsed = JSON.parse(extractedJson);
        const validationResult = validateAnalysisStructure(parsed);
        
        if (validationResult.isValid) {
          const parseTime = Date.now() - startTime;
          trackJsonParsing(model, stage, 'smart-pattern', true, responseText.length, parseTime);
          console.log('‚úÖ JSON extracted and validated successfully');
          return { success: true, data: parsed };
        } else {
          console.log('‚ö†Ô∏è Pattern matched but validation failed:', validationResult.errors);
        }
      } catch (parseError) {
        console.log('‚ùå Pattern matched but JSON invalid');
        continue;
      }
    }
    // Reset regex lastIndex for next pattern
    pattern.lastIndex = 0;
  }
  
  // Method 3: Content cleaning and repair
  try {
    let cleaned = responseText.trim();
    
    // Remove markdown formatting
    cleaned = cleaned.replace(/```json\s*/gi, '').replace(/```\s*/g, '');
    
    // Remove common AI response prefixes
    cleaned = cleaned.replace(/^(Here's|Here is|The analysis|Analysis:|Result:)[\s\S]*?(\{)/i, '{');
    
    // Find first { and last }
    const firstBrace = cleaned.indexOf('{');
    const lastBrace = cleaned.lastIndexOf('}');
    
    if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
      cleaned = cleaned.substring(firstBrace, lastBrace + 1);
      
      // Fix common JSON issues
      cleaned = cleaned
        .replace(/([{,]\s*)(\w+):/g, '$1"$2":')        // Quote unquoted keys
        .replace(/:\s*'([^']*?)'/g, ': "$1"')          // Replace single quotes
        .replace(/,\s*}/g, '}')                        // Remove trailing commas
        .replace(/,\s*]/g, ']');                       // Remove trailing commas in arrays
      
      const parsed = JSON.parse(cleaned);
      const validationResult = validateAnalysisStructure(parsed);
      
      if (validationResult.isValid) {
        const parseTime = Date.now() - startTime;
        trackJsonParsing(model, stage, 'content-cleaning', true, responseText.length, parseTime);
        console.log('‚úÖ JSON parsed after cleaning and repair');
        return { success: true, data: parsed };
      }
    }
  } catch (cleanError) {
    console.log('‚ùå Content cleaning and repair failed');
  }
  
  // Method 4: Generate fallback response based on any extractable content
  console.log('üîÑ Generating fallback response from content analysis');
  const fallbackData = generateFallbackAnalysis(responseText);
  const parseTime = Date.now() - startTime;
  trackJsonParsing(model, stage, 'fallback-generated', true, responseText.length, parseTime);
  console.log('‚úÖ Fallback analysis generated from content');
  return { success: true, data: fallbackData };
}

// PHASE 1: Analysis structure validation
function validateAnalysisStructure(data: any): { isValid: boolean; errors: string[] } {
  const errors: string[] = [];
  
  if (!data || typeof data !== 'object') {
    errors.push('Data is not an object');
    return { isValid: false, errors };
  }
  
  // Check for essential analysis fields
  if (!data.suggestions && !data.visual_annotations && !data.summary) {
    errors.push('Missing core analysis fields (suggestions, visual_annotations, summary)');
  }
  
  // Validate suggestions structure
  if (data.suggestions && !Array.isArray(data.suggestions)) {
    errors.push('Suggestions must be an array');
  } else if (data.suggestions && data.suggestions.length > 0) {
    const firstSuggestion = data.suggestions[0];
    if (!firstSuggestion.title && !firstSuggestion.description) {
      errors.push('Suggestions lack required fields (title, description)');
    }
  }
  
  // Validate summary structure
  if (data.summary && typeof data.summary === 'object') {
    if (typeof data.summary.overallScore !== 'number' && typeof data.summary.overall_score !== 'number') {
      errors.push('Summary missing overall score');
    }
  }
  
  return { isValid: errors.length === 0, errors };
}

// PHASE 1: Fallback analysis generator
function generateFallbackAnalysis(responseText: string): any {
  console.log('üîÑ Generating fallback analysis from response content');
  
  // Try to extract any meaningful content from the response
  const content = responseText.toLowerCase();
  const words = responseText.split(/\s+/);
  
  // Generate basic analysis based on content analysis
  const suggestions = [];
  const keyIssues = [];
  const strengths = [];
  
  // Look for common UX terms and issues
  if (content.includes('accessibility') || content.includes('a11y')) {
    suggestions.push({
      id: 'accessibility-improvement',
      category: 'accessibility',
      title: 'Accessibility Enhancement',
      description: 'Consider improving accessibility based on analysis content',
      impact: 'high',
      effort: 'medium',
      actionItems: ['Review accessibility guidelines', 'Test with screen readers']
    });
    keyIssues.push('Accessibility considerations identified');
  }
  
  if (content.includes('usability') || content.includes('user experience')) {
    suggestions.push({
      id: 'usability-improvement',
      category: 'usability', 
      title: 'Usability Enhancement',
      description: 'Usability improvements identified in analysis',
      impact: 'medium',
      effort: 'medium',
      actionItems: ['Conduct user testing', 'Review interaction patterns']
    });
  }
  
  if (content.includes('visual') || content.includes('design')) {
    suggestions.push({
      id: 'visual-improvement',
      category: 'visual',
      title: 'Visual Design Enhancement', 
      description: 'Visual design improvements suggested',
      impact: 'medium',
      effort: 'low',
      actionItems: ['Review visual hierarchy', 'Check color contrast']
    });
  }
  
  // If no specific suggestions found, add generic ones
  if (suggestions.length === 0) {
    suggestions.push({
      id: 'general-improvement',
      category: 'usability',
      title: 'Analysis Incomplete',
      description: 'AI analysis was incomplete. Please try again with a different approach.',
      impact: 'low',
      effort: 'low',
      actionItems: ['Retry analysis', 'Check image clarity']
    });
    keyIssues.push('Analysis was incomplete or unclear');
  } else {
    strengths.push('Image was successfully analyzed');
  }
  
  return {
    suggestions,
    visual_annotations: [],
    summary: {
      overallScore: Math.max(40, Math.min(60, 50 + (strengths.length * 5) - (keyIssues.length * 5))),
      categoryScores: {
        usability: 50,
        accessibility: 45,
        visual: 55,
        content: 50
      },
      keyIssues,
      strengths
    },
    metadata: {
      objects: [],
      text: [],
      colors: [],
      faces: 0,
      fallbackGenerated: true,
      originalResponseLength: responseText.length
    }
  };
}

// PHASE 4.2: Get parsing metrics for monitoring endpoint
function getParsingMetrics() {
  const totalAttempts = parsingMetrics.length;
  const successfulAttempts = parsingMetrics.filter(m => m.success).length;
  const successRate = totalAttempts > 0 ? successfulAttempts / totalAttempts : 0;

  // Method breakdown
  const methodBreakdown: Record<string, number> = {};
  parsingMetrics.forEach(m => {
    methodBreakdown[m.method] = (methodBreakdown[m.method] || 0) + 1;
  });

  // Model performance breakdown
  const modelPerformance: Record<string, { success: number; total: number; rate: number }> = {};
  parsingMetrics.forEach(m => {
    if (!modelPerformance[m.model]) {
      modelPerformance[m.model] = { success: 0, total: 0, rate: 0 };
    }
    modelPerformance[m.model].total++;
    if (m.success) {
      modelPerformance[m.model].success++;
    }
  });

  // Calculate rates
  Object.keys(modelPerformance).forEach(model => {
    const perf = modelPerformance[model];
    perf.rate = perf.total > 0 ? perf.success / perf.total : 0;
  });

  // Average parse time
  const parseTimes = parsingMetrics.map(m => m.parseTime);
  const averageParseTime = parseTimes.length > 0 
    ? parseTimes.reduce((a, b) => a + b, 0) / parseTimes.length 
    : 0;

  return {
    totalAttempts,
    successRate: Math.round(successRate * 100) / 100,
    methodBreakdown,
    modelPerformance,
    averageParseTime: Math.round(averageParseTime * 100) / 100,
    timestamp: new Date().toISOString()
  };
}

// Pre-flight validation for JSON prompt requirements
function validateJsonPromptRequirement(payload: any): { isValid: boolean; error?: string; fixedPrompt?: string } {
  const { prompt, model } = payload;
  
  // Check if this is an OpenAI model that might use JSON mode
  const isOpenAIModel = model && (model.includes('gpt-') || model.includes('openai'));
  
  if (!isOpenAIModel) {
    return { isValid: true }; // Non-OpenAI models don't need this validation
  }
  
  // Check if prompt contains "json" keyword
  const hasJsonKeyword = prompt.toLowerCase().includes('json');
  
  if (!hasJsonKeyword) {
    console.log('üîß Pre-flight validation: JSON keyword missing, auto-fixing prompt');
    const fixedPrompt = ensureJsonInPrompt(prompt);
    return { 
      isValid: true, 
      fixedPrompt,
      error: `Original prompt lacked "json" keyword required for OpenAI JSON mode. Auto-fixed.`
    };
  }
  
  console.log('‚úÖ Pre-flight validation: JSON keyword present in prompt');
  return { isValid: true };
}

async function executeOpenAI(config: any, apiKey: string, payload: any) {
  console.log('Executing OpenAI with payload:', { 
    model: config.model || payload.model,
    stage: payload.stage,
    hasImage: !!(payload.imageUrl || payload.imageBase64),
    promptLength: payload.prompt?.length || 0
  });
  
  
  try {
    const { imageUrl, imageBase64, systemPrompt } = payload;
    
    // PHASE 2.1: Pre-flight validation with auto-fixing
    const validation = validateJsonPromptRequirement(payload);
    const promptToUse = validation.fixedPrompt || payload.prompt;
    
    if (validation.error) {
      console.log('‚ö†Ô∏è JSON validation warning:', validation.error);
    }
    
    // Additional logging for debugging
    console.log('üîç JSON prompt check:', {
      originalContainsJson: payload.prompt.toLowerCase().includes('json'),
      finalPromptContainsJson: promptToUse.toLowerCase().includes('json'),
      promptWasModified: promptToUse !== payload.prompt,
      validationPassed: validation.isValid
    });
    
    let processedImageUrl = null;
    
    // Handle image data - prioritize base64 from frontend over URL fetching
    if (imageBase64) {
      console.log('Using provided base64 image data, length:', imageBase64.length);
      processedImageUrl = `data:image/jpeg;base64,${imageBase64}`;
    } else if (imageUrl && !imageUrl.startsWith('data:')) {
      console.log('Converting image URL to base64 for OpenAI...');
      const base64Data = await fetchImageAsBase64(imageUrl);
      processedImageUrl = `data:image/jpeg;base64,${base64Data}`;
      console.log('Image converted successfully');
    } else if (imageUrl && imageUrl.startsWith('data:')) {
      console.log('Using existing data URL');
      processedImageUrl = imageUrl;
    }
    
    const messages = [
      {
        role: 'system',
        content: systemPrompt || 'You are an expert UX/UI analyst.'
      },
      {
        role: 'user',
        content: processedImageUrl ? [
          { type: 'text', text: promptToUse },
          { type: 'image_url', image_url: { url: processedImageUrl } }
        ] : promptToUse
      }
    ];

    console.log('Sending request to OpenAI API...');
    const response = await fetch(config.endpoint, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: config.model || payload.model,
        messages,
        max_tokens: 4000,
        temperature: 0.3,
        response_format: { type: 'json_object' }
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('OpenAI API error:', response.status, response.statusText, errorText);
      
      // PHASE 2.2: Enhanced error handling for JSON format issues
      if (response.status === 400 && errorText.includes('json')) {
        const enhancedError = `OpenAI JSON Format Error: ${errorText}
        
üîß Troubleshooting Steps:
1. Ensure your prompt contains the word "json" when using JSON mode
2. Original prompt contained JSON keyword: ${payload.prompt.toLowerCase().includes('json')}
3. Final prompt contained JSON keyword: ${promptToUse.toLowerCase().includes('json')}
4. Prompt was auto-fixed: ${promptToUse !== payload.prompt}

üí° This error typically occurs when response_format: { type: 'json_object' } is used without mentioning "json" in the prompt.
The system attempted to auto-fix this issue. If you continue seeing this error, please check the prompt content.`;

        throw new Error(enhancedError);
      }
      
      throw new Error(`OpenAI API error: ${response.status} ${response.statusText}: ${errorText}`);
    }

    const data = await response.json();
    console.log('OpenAI response received successfully');
    const content = data.choices[0].message.content;
    
    // PHASE 3.2: Hybrid Strategy - Try strict JSON first, fallback to smart parsing
    try {
      // Method 1: Direct JSON parsing (for proper JSON responses)
      const parsedJson = JSON.parse(content);
      console.log('‚úÖ PHASE 3.2: Direct JSON parsing successful');
      return new Response(
        JSON.stringify(parsedJson),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    } catch (directParseError) {
      console.log('‚ö†Ô∏è PHASE 3.2: Direct JSON parse failed, attempting smart parsing');
      
      // Method 2: Smart parsing fallback
      const smartParseResult = parseJsonFromResponse(content, payload.model, payload.stage);
      
      if (smartParseResult.success) {
        console.log('‚úÖ PHASE 3.2: Smart parsing successful');
        return new Response(
          JSON.stringify(smartParseResult.data),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      } else {
        console.error('‚ùå PHASE 3.2: Both parsing methods failed:', smartParseResult.error);
        // Return raw content with error info
        return new Response(
          JSON.stringify({ 
            content, 
            parseError: smartParseResult.error,
            note: 'Raw response returned due to JSON parsing failure'
          }),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
    }
  } catch (error) {
    console.error('Error in executeOpenAI:', error);
    throw error;
  }
}

async function executeAnthropic(config: any, apiKey: string, payload: any) {
  console.log('Executing Anthropic with payload:', { 
    model: config.model || payload.model,
    stage: payload.stage,
    hasImage: !!(payload.imageUrl || payload.imageBase64),
    promptLength: payload.prompt?.length || 0
  });
  
  try {
    const { imageUrl, imageBase64, prompt, systemPrompt } = payload;
    
    let base64Data = '';
    let mediaType = 'image/jpeg';
    
    // Handle image data - prioritize base64 from frontend over URL fetching
    if (imageBase64) {
      console.log('Using provided base64 image data, length:', imageBase64.length);
      base64Data = imageBase64;
      mediaType = 'image/jpeg'; // Default, could be enhanced to detect format
    } else if (imageUrl && !imageUrl.startsWith('data:')) {
      console.log('Converting image URL to base64 for Anthropic...');
      base64Data = await fetchImageAsBase64(imageUrl);
      mediaType = detectImageFormat(imageUrl, base64Data);
      console.log('Image converted successfully for Anthropic');
    } else if (imageUrl && imageUrl.startsWith('data:')) {
      console.log('Using existing data URL for Anthropic');
      base64Data = imageUrl.split(',')[1];
      mediaType = detectImageFormat(imageUrl);
    }
    
    const messages = [{
      role: 'user',
      content: imageUrl ? [
        { type: 'text', text: prompt },
          { 
            type: 'image', 
            source: {
              type: 'base64',
              media_type: detectImageFormat(imageUrl, base64Data),
              data: imageUrl.startsWith('data:') ? imageUrl.split(',')[1] : base64Data
            }
          }
      ] : prompt
    }];

    console.log('Sending request to Anthropic API...');
    const response = await fetch(config.endpoint, {
      method: 'POST',
      headers: {
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: config.model || payload.model,
        system: systemPrompt || 'You are an expert UX/UI analyst.',
        messages,
        max_tokens: 4000,
        temperature: 0.3
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Anthropic API error:', response.status, response.statusText, errorText);
      throw new Error(`Anthropic API error: ${response.status} ${response.statusText}: ${errorText}`);
    }

    const data = await response.json();
    console.log('Anthropic response received successfully');
    const content = data.content[0].text;
    
    // PHASE 3.2: Apply hybrid strategy to Anthropic as well
    try {
      // Method 1: Direct JSON parsing
      const parsedJson = JSON.parse(content);
      console.log('‚úÖ Anthropic: Direct JSON parsing successful');
      return new Response(
        JSON.stringify(parsedJson),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    } catch (directParseError) {
      console.log('‚ö†Ô∏è Anthropic: Direct JSON parse failed, attempting smart parsing');
      
      // Method 2: Smart parsing fallback
      const smartParseResult = parseJsonFromResponse(content, payload.model, payload.stage);
      
      if (smartParseResult.success) {
        console.log('‚úÖ Anthropic: Smart parsing successful');
        return new Response(
          JSON.stringify(smartParseResult.data),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      } else {
        console.log('‚ö†Ô∏è Anthropic: Smart parsing failed, returning raw content');
        return new Response(
          JSON.stringify({ content }),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
    }
  } catch (error) {
    console.error('Error in executeAnthropic:', error);
    throw error;
  }
}

async function executePerplexity(config: any, apiKey: string, payload: any) {
  const { prompt, systemPrompt } = payload
  
  const response = await fetch(config.endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: config.model,
      messages: [
        {
          role: 'system',
          content: systemPrompt || 'You are a research assistant providing current UX/UI insights and best practices.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 2000,
      temperature: 0.2
    })
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`Perplexity API error: ${error}`)
  }

  const data = await response.json()
  const content = data.choices[0].message.content
  
  return new Response(
    JSON.stringify({ content, citations: data.citations || [] }),
    { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
  )
}

async function executeStability(config: any, apiKey: string, payload: any) {
  const { imageUrl, prompt, mask } = payload
  
  // For inpainting, we need to handle form data differently
  const formData = new FormData()
  
  // Fetch the original image
  const imageResponse = await fetch(imageUrl)
  const imageBlob = await imageResponse.blob()
  formData.append('init_image', imageBlob)
  
  if (mask) {
    const maskBlob = new Blob([mask], { type: 'image/png' })
    formData.append('mask_image', maskBlob)
  }
  
  formData.append('text_prompts[0][text]', prompt)
  formData.append('text_prompts[0][weight]', '1')
  formData.append('cfg_scale', '7')
  formData.append('steps', '30')
  
  const response = await fetch(config.endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
    },
    body: formData
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`Stability AI API error: ${error}`)
  }

  const data = await response.json()
  
  return new Response(
    JSON.stringify({ 
      imageBase64: data.artifacts[0].base64,
      finishReason: data.artifacts[0].finishReason
    }),
    { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
  )
}

// Model name mapping function
function mapModelName(requestedModel: string): string {
  const modelMapping: Record<string, string> = {
    // OpenAI model mappings
    'gpt-4.1-2025-04-14': 'gpt-4o',
    'gpt-4o': 'gpt-4o',
    'gpt-4o-mini': 'gpt-4o-mini',
    
    
    // Anthropic model mappings
    'claude-opus-4-20250514': 'claude-opus-4-20250514',
    'claude-3-5-sonnet-20241022': 'claude-opus-4-20250514',
    'claude-3-opus-20240229': 'claude-opus-4-20250514',
    
    // Note: Google Vision is handled separately for metadata extraction only
    // It should NOT appear in vision/analysis pipelines
    
    // Perplexity mappings
    'perplexity-sonar': 'llama-3.1-sonar-small-128k-online'
  }
  
  const mapped = modelMapping[requestedModel] || requestedModel
  console.log(`Model mapping: ${requestedModel} -> ${mapped}`)
  return mapped
}

// Find best model match for suggestions
function findBestModelMatch(requestedModel: string, availableModels: string[]): string {
  // Handle "auto" model selection
  if (requestedModel === 'auto') {
    return 'gpt-4o'
  }
  
  // Simple fuzzy matching
  if (requestedModel.includes('gpt') || requestedModel.includes('openai')) {
    return availableModels.find(m => m.includes('gpt')) || 'gpt-4o'
  }
  if (requestedModel.includes('claude') || requestedModel.includes('anthropic')) {
    return availableModels.find(m => m.includes('claude')) || 'claude-opus-4-20250514'
  }
  if (requestedModel.includes('gemini') || requestedModel.includes('google')) {
    return availableModels.find(m => m.includes('gemini')) || 'gemini-2.5-pro'
  }
  return 'gpt-4o'
}

// Image format detection function
function detectImageFormat(imageUrl: string, base64Data?: string): string {
  // Check data URL format first
  if (imageUrl.startsWith('data:image/')) {
    const format = imageUrl.match(/data:image\/([^;]+)/)?.[1]
    if (format) {
      return `image/${format}`
    }
  }
  
  // Check file extension
  const urlLower = imageUrl.toLowerCase()
  if (urlLower.includes('.png')) return 'image/png'
  if (urlLower.includes('.webp')) return 'image/webp'
  if (urlLower.includes('.gif')) return 'image/gif'
  if (urlLower.includes('.svg')) return 'image/svg+xml'
  
  // Check base64 data if available
  if (base64Data) {
    // PNG starts with iVBORw0KGgo
    if (base64Data.startsWith('iVBORw0KGgo')) return 'image/png'
    // WebP starts with UklGR
    if (base64Data.startsWith('UklGR')) return 'image/webp'
    // GIF starts with R0lGODlh or R0lGODdh
    if (base64Data.startsWith('R0lGOD')) return 'image/gif'
  }
  
  // Default to JPEG if unable to detect
  console.warn(`Could not detect image format for ${imageUrl}, defaulting to JPEG`)
  return 'image/jpeg'
}

function buildAnalysisPrompt(userContext?: string): string {
  const contextPart = userContext ? `Context: ${userContext}` : 'General UX analysis';
  
  return `Analyze this application interface for usability, navigation patterns, and feature discoverability. Document the interaction model, state management, and user workflows.

Designer Perspective:
- Evaluate visual hierarchy and gestalt principles
- Analyze color theory application and accessibility
- Assess typography system and readability
- Review spacing consistency and visual rhythm
- Consider emotional design impact
- Identify design system opportunities

Perform comprehensive UX analysis of this interface, identifying its purpose, user flows, and optimization opportunities.

Return analysis as structured JSON with:
{
  "elements": { /* UI elements detected */ },
  "layout": { /* Grid, spacing, hierarchy */ },
  "colors": { /* Color palette and usage */ },
  "content": { /* Text content and headings */ },
  "interactions": { /* Buttons, links, forms */ },
  "confidence": { /* Detection confidence scores */ }
}

Quality Requirements:
- Provide specific, measurable recommendations
- Include confidence scores for all assessments
- Reference established UX principles and guidelines
- Consider both user needs and business objectives
- Ensure compliance with WCAG-AA standards
- Prioritize recommendations by effort

${contextPart}`;
}

async function fetchImageAsBase64(imageUrl: string): Promise<string> {
  console.log('Fetching image from URL:', imageUrl);
  
  try {
    const response = await fetch(imageUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch image: ${response.status} ${response.statusText}`);
    }
    
    const arrayBuffer = await response.arrayBuffer();
    console.log('Image fetched successfully, size:', arrayBuffer.byteLength, 'bytes');
    
    // Convert ArrayBuffer to base64 using Deno's built-in btoa
    const uint8Array = new Uint8Array(arrayBuffer);
    const binaryString = Array.from(uint8Array, byte => String.fromCharCode(byte)).join('');
    const base64String = btoa(binaryString);
    
    console.log('Base64 conversion complete, length:', base64String.length);
    return base64String;
  } catch (error) {
    console.error('Error in fetchImageAsBase64:', error);
    throw new Error(`Failed to convert image to base64: ${error.message}`);
  }
}

async function handleCanvasRequest(action: string, payload: any) {
  console.log('Handling Canvas request - Action:', action, 'Payload:', payload)
  
  // Convert old Canvas format to new pipeline format
  switch (action) {
    case 'ANALYZE_IMAGE':
      try {
        // Execute full multi-model pipeline with Google Vision metadata
        const analysisResult = await executeMultiModelPipeline({
          imageUrl: payload.payload?.imageUrl,
          imageBase64: payload.payload?.imageBase64,
          userContext: payload.payload?.userContext,
          imageName: payload.payload?.imageName,
          imageId: payload.payload?.imageId
        })
        
        console.log('‚úÖ Multi-model pipeline result:', { 
          hasData: !!analysisResult,
          hasVision: !!analysisResult.visionMetadata,
          hasOpenAI: !!analysisResult.openaiAnalysis,
          hasClaude: !!analysisResult.claudeAnalysis,
          hasSynthesis: !!analysisResult.synthesizedResult
        })

        // CRITICAL FIX: Save analysis to database
        try {
          if (payload.payload?.imageId && analysisResult.synthesizedResult) {
            console.log('üíæ Saving analysis to database...');
            const analysisDataWithMetadata = {
              ...analysisResult.synthesizedResult,
              visionMetadata: analysisResult.visionMetadata
            };
            await saveAnalysisToDatabase(payload.payload.imageId, analysisDataWithMetadata);
            console.log('‚úÖ Analysis saved to database successfully');
          }
        } catch (saveError) {
          console.error('‚ö†Ô∏è Failed to save analysis to database:', saveError);
          // Don't fail the request if database save fails
        }
        
        // Return synthesized analysis in Canvas-expected format
        return new Response(
          JSON.stringify({
            success: true,
            data: analysisResult.synthesizedResult
          }),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        )
      } catch (error) {
        console.error('Error in Canvas request:', error)
        return new Response(
          JSON.stringify({
            success: false,
            error: error.message
          }),
          { 
            status: 500,
            headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
          }
        )
      }
      
    default:
      throw new Error(`Unknown Canvas action: ${action}`)
  }
}

// PHASE 1: Enhanced analysis storage with comprehensive validation
async function storeAnalysisResults(pipelineResults: any) {
  console.log('üîç PHASE 1: Enhanced analysis storage with validation');
  
  try {
    const { visionResults, analysisResults, synthesisResults, analysisContext } = pipelineResults;
    
    // PHASE 1: Pre-validation checks
    if (!synthesisResults && !analysisResults && !visionResults) {
      throw new Error('No valid analysis results to store');
    }
    
    // PHASE 1: Smart data extraction with fallbacks
    const extractSuggestions = () => {
      if (synthesisResults?.recommendations && Array.isArray(synthesisResults.recommendations)) {
        return synthesisResults.recommendations.map((rec: any, index: number) => ({
          id: rec.id || `suggestion_${index}`,
          category: rec.category || 'usability',
          title: rec.title || 'Improvement Suggestion',
          description: rec.description || 'Analysis identified an area for improvement',
          impact: rec.impact || rec.priority || 'medium',
          effort: rec.effort || 'medium',
          actionItems: Array.isArray(rec.actionItems) ? rec.actionItems : 
                      Array.isArray(rec.action_items) ? rec.action_items : 
                      [rec.description || 'Review this area'],
          relatedAnnotations: []
        }));
      }
      
      // Fallback: try to extract from analysisResults
      if (analysisResults?.suggestions && Array.isArray(analysisResults.suggestions)) {
        return analysisResults.suggestions;
      }
      
      // Final fallback: extract from any available data
      if (analysisResults?.fusedData?.recommendations) {
        return analysisResults.fusedData.recommendations;
      }
      
      return [];
    };
    
    const extractAnnotations = () => {
      if (synthesisResults?.recommendations && Array.isArray(synthesisResults.recommendations)) {
        return synthesisResults.recommendations.map((rec: any, index: number) => ({
          id: `annotation_${index}`,
          x: 10 + (index * 25),
          y: 10 + (index * 20),
          type: rec.priority === 'critical' || rec.impact === 'high' ? 'issue' : 'suggestion',
          title: rec.title || 'UX Issue',
          description: rec.description || 'Area for improvement identified',
          severity: rec.priority || rec.impact || 'medium'
        }));
      }
      return [];
    };
    
    const extractSummary = () => {
      const defaultSummary = {
        overallScore: 65,
        categoryScores: {
          usability: 60,
          accessibility: 55,
          visual: 70,
          content: 65
        },
        keyIssues: [],
        strengths: []
      };
      
      // Try to extract from synthesis results
      if (synthesisResults?.executiveSummary) {
        const exec = synthesisResults.executiveSummary;
        return {
          overallScore: typeof exec.overallScore === 'number' ? exec.overallScore : defaultSummary.overallScore,
          categoryScores: {
            usability: analysisResults?.fusedData?.usabilityScore || defaultSummary.categoryScores.usability,
            accessibility: analysisResults?.fusedData?.accessibilityScore || defaultSummary.categoryScores.accessibility,
            visual: (visionResults?.fusedData?.confidence?.overall || 0.7) * 100,
            content: defaultSummary.categoryScores.content
          },
          keyIssues: Array.isArray(exec.criticalIssues) ? exec.criticalIssues : 
                    Array.isArray(exec.keyIssues) ? exec.keyIssues : [],
          strengths: Array.isArray(exec.strengths) ? exec.strengths :
                    Array.isArray(analysisResults?.fusedData?.strengths) ? analysisResults.fusedData.strengths : []
        };
      }
      
      // Try to extract from analysis results
      if (analysisResults?.summary) {
        return {
          ...defaultSummary,
          ...analysisResults.summary,
          categoryScores: {
            ...defaultSummary.categoryScores,
            ...analysisResults.summary.categoryScores
          }
        };
      }
      
      return defaultSummary;
    };
    
    // PHASE 1: Create comprehensive analysis structure
    const rawAnalysis = {
      visualAnnotations: extractAnnotations(),
      suggestions: extractSuggestions(),
      summary: extractSummary(),
      metadata: {
        timestamp: new Date().toISOString(),
        modelsUsed: pipelineResults.modelsUsed || ['enhanced-pipeline'],
        executionTime: pipelineResults.executionTime || 0,
        confidence: visionResults?.confidence || 0.75,
        pipelineVersion: '2.1-enhanced',
        stagesCompleted: ['context', 'vision', 'analysis', 'synthesis'],
        aiGenerated: true,
        qualityScore: calculateQualityScore(extractSuggestions(), extractSummary()),
        objects: visionResults?.fusedData?.objects || [],
        text: visionResults?.fusedData?.text || [],
        colors: visionResults?.fusedData?.colors || [],
        faces: visionResults?.fusedData?.faces || 0
      },
      analysisContext: analysisContext || null
    };
    
    // PHASE 1: Apply comprehensive validation
    console.log('üîç Applying comprehensive validation...');
    const validationResult = AnalysisValidator.validateAndNormalize(rawAnalysis);
    
    if (!validationResult.isValid) {
      console.warn('‚ö†Ô∏è Analysis validation failed, using corrected data');
    }
    
    if (validationResult.warnings.length > 0) {
      console.warn('‚ö†Ô∏è Analysis validation warnings:', validationResult.warnings);
    }
    
    // PHASE 1: Quality check - ensure minimum viable analysis
    const finalData = ensureMinimumViableAnalysis(validationResult.data);
    
    console.log('‚úÖ Analysis storage complete:', {
      suggestionsCount: finalData.suggestions?.length || 0,
      annotationsCount: finalData.visualAnnotations?.length || 0,
      overallScore: finalData.summary?.overallScore || 'N/A',
      warningsCount: validationResult.warnings.length
    });

    return new Response(
      JSON.stringify({
        success: true,
        data: finalData,
        warnings: validationResult.warnings,
        qualityMetrics: {
          dataCompleteness: calculateDataCompleteness(finalData),
          confidence: finalData.metadata?.confidence || 0.75
        }
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
    
  } catch (error) {
    console.error('‚ùå Analysis storage failed:', error);
    
    // PHASE 1: Generate emergency fallback analysis
    const fallbackAnalysis = generateEmergencyFallbackAnalysis();
    
    return new Response(
      JSON.stringify({
        success: true,
        data: fallbackAnalysis,
        warnings: [`Analysis storage failed: ${error.message}`, 'Using emergency fallback analysis'],
        isFallback: true
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
}

// PHASE 1: Helper functions for enhanced storage
function calculateQualityScore(suggestions: any[], summary: any): number {
  let score = 50; // Base score
  
  if (suggestions && suggestions.length > 0) {
    score += Math.min(suggestions.length * 5, 25); // Up to 25 points for suggestions
  }
  
  if (summary) {
    if (typeof summary.overallScore === 'number') score += 10;
    if (summary.categoryScores && Object.keys(summary.categoryScores).length > 0) score += 10;
    if (Array.isArray(summary.keyIssues) && summary.keyIssues.length > 0) score += 5;
  }
  
  return Math.min(score, 100);
}

function calculateDataCompleteness(data: any): number {
  let completeness = 0;
  const checks = [
    () => Array.isArray(data.suggestions) && data.suggestions.length > 0,
    () => Array.isArray(data.visualAnnotations),
    () => data.summary && typeof data.summary.overallScore === 'number',
    () => data.summary && data.summary.categoryScores,
    () => data.metadata && data.metadata.timestamp,
    () => Array.isArray(data.summary?.keyIssues),
    () => Array.isArray(data.summary?.strengths)
  ];
  
  checks.forEach(check => {
    if (check()) completeness += (100 / checks.length);
  });
  
  return Math.round(completeness);
}

function ensureMinimumViableAnalysis(data: any): any {
  // Ensure minimum viable analysis for display
  if (!data.suggestions || data.suggestions.length === 0) {
    data.suggestions = [{
      id: 'minimum-suggestion',
      category: 'usability',
      title: 'Analysis Completed',
      description: 'AI analysis was completed successfully. Consider reviewing the interface for potential improvements.',
      impact: 'low',
      effort: 'low',
      actionItems: ['Review interface elements', 'Consider user feedback']
    }];
  }
  
  if (!data.summary || typeof data.summary.overallScore !== 'number') {
    if (!data.summary) data.summary = {};
    data.summary.overallScore = 65;
  }
  
  if (!data.summary.categoryScores) {
    data.summary.categoryScores = {
      usability: 60,
      accessibility: 55,
      visual: 70,
      content: 65
    };
  }
  
  return data;
}

function generateEmergencyFallbackAnalysis(): any {
  return {
    suggestions: [{
      id: 'emergency-fallback',
      category: 'usability',
      title: 'Analysis System Recovery',
      description: 'The analysis system encountered an issue but has recovered. Please try analyzing the image again for full insights.',
      impact: 'low',
      effort: 'low',
      actionItems: ['Retry analysis', 'Check image quality', 'Verify internet connection']
    }],
    visualAnnotations: [],
    summary: {
      overallScore: 50,
      categoryScores: {
        usability: 50,
        accessibility: 50,
        visual: 50,
        content: 50
      },
      keyIssues: ['Analysis system encountered an issue'],
      strengths: ['System successfully recovered']
    },
    metadata: {
      timestamp: new Date().toISOString(),
      modelsUsed: ['emergency-fallback'],
      executionTime: 0,
      confidence: 0.3,
      pipelineVersion: '2.1-emergency',
      stagesCompleted: ['emergency-recovery'],
      aiGenerated: false,
      isEmergencyFallback: true
    }
  };
}

// ============= MULTI-MODEL PIPELINE =============

async function executeMultiModelPipeline(params: {
  imageUrl: string;
  imageBase64?: string;
  userContext?: string;
  imageName?: string;
  imageId?: string;
  enhancedContext?: {
    analysisContext?: any;
    metadata?: any;
    contextualPrompt?: string;
  };
}): Promise<any> {
  console.log('üöÄ Starting multi-model pipeline with Google Vision + OpenAI + Claude');
  
  const { imageUrl, imageBase64, userContext, imageName, imageId, enhancedContext } = params;
  
  try {
    // Step 0: Context Detection - use enhanced context if available
    let contextResult;
    let visionMetadata;
    
    if (enhancedContext?.analysisContext) {
      console.log('üéØ Using enhanced context from EnhancedAnalysisPipeline');
      contextResult = {
        analysisContext: enhancedContext.analysisContext,
        requiresClarification: false
      };
      visionMetadata = enhancedContext.metadata || {};
    } else {
      console.log('üîç Step 0: Detecting context and checking for clarification needs...');
      contextResult = await detectImageContextAndClarification(imageUrl, imageBase64, userContext);
      
      // If clarification is needed, return early
      if (contextResult.requiresClarification) {
        console.log('‚ùì Context clarification needed, returning early');
        return {
          success: true,
          data: {
            requiresClarification: true,
            questions: contextResult.questions,
            partialContext: contextResult.analysisContext
          }
        };
      }
      
      console.log('‚úÖ Context detection complete, proceeding with analysis');
      
      // Step 1: Extract metadata with Google Vision service
      console.log('üìä Step 1: Extracting metadata with Google Vision service...');
      visionMetadata = await extractGoogleVisionMetadata(imageUrl, imageBase64);
    }
    
    // Step 2: Run parallel analysis with OpenAI and Claude, passing vision metadata
    console.log('ü§ñ Step 2: Running parallel analysis with OpenAI + Claude...');
    
    // Use Promise.allSettled to handle partial failures gracefully
    const results = await Promise.allSettled([
      runOpenAIAnalysis(imageUrl, imageBase64, userContext, visionMetadata, enhancedContext),
      runClaudeAnalysis(imageUrl, imageBase64, userContext, visionMetadata, enhancedContext)
    ]);
    
    const openaiAnalysis = results[0].status === 'fulfilled' ? results[0].value : null;
    const claudeAnalysis = results[1].status === 'fulfilled' ? results[1].value : null;
    
    // Log any failures for debugging
    if (results[0].status === 'rejected') {
      console.error('OpenAI analysis failed:', results[0].reason);
    }
    if (results[1].status === 'rejected') {
      console.error('Claude analysis failed:', results[1].reason);
    }
    
    // Ensure we have at least one successful analysis
    if (!openaiAnalysis && !claudeAnalysis) {
      throw new Error('Both OpenAI and Claude analysis failed');
    }
    
    console.log('‚úÖ Analysis results:', {
      openaiSuccess: !!openaiAnalysis,
      claudeSuccess: !!claudeAnalysis
    });
    
    // Step 3: Synthesize results from both models
    console.log('üîÆ Step 3: Synthesizing multi-model results...');
    const synthesizedResult = await synthesizeMultiModelResults(
      openaiAnalysis,
      claudeAnalysis,
      visionMetadata,
      { imageName, imageId, userContext }
    );
    
    console.log('‚úÖ Multi-model pipeline complete');
    return {
      visionMetadata,
      openaiAnalysis,
      claudeAnalysis,
      synthesizedResult
    };
    
  } catch (error) {
    console.error('‚ùå Multi-model pipeline error:', error);
    throw error;
  }
}

// CRITICAL: Context Detection Function that was missing
async function detectImageContextAndClarification(imageUrl: string, imageBase64?: string, userContext?: string): Promise<any> {
  console.log('[ContextDetection] Starting image context detection...');
  
  try {
    const contextPrompt = `Analyze this UI/UX interface image and determine:
    1. Primary interface type (dashboard/landing/app/form/ecommerce/content/portfolio/saas/mobile)
    2. Sub-types or specific patterns
    3. Domain/industry (finance/healthcare/education/retail/etc)
    4. Complexity level (simple/moderate/complex)
    5. Likely user intents based on UI elements
    6. Business model indicators
    7. Target audience indicators
    8. Product maturity stage
    9. Platform type (web/mobile/desktop/responsive)
    10. Design system presence and consistency

    Please return your analysis as a valid JSON object with confidence scores for each determination.`;

    const payload = {
      model: 'gpt-4o',
      imageUrl,
      imageBase64,
      prompt: contextPrompt,
      maxTokens: 1000
    };

    console.log('[ContextDetection] Calling OpenAI for context detection...');
    const response = await executeOpenAI(MODEL_CONFIGS['gpt-4o'], Deno.env.get('OPENAI_API_KEY')!, payload);
    const responseText = await response.text();
    const contextData = JSON.parse(responseText);

    console.log('[ContextDetection] Successfully received context data:', contextData);
    
    // Parse context and create analysis context
    const imageContext = parseImageContext(contextData);
    const userContextParsed = inferUserContext(userContext || '');
    const analysisContext = createAnalysisContext(imageContext, userContextParsed);
    
    console.log('[ContextDetection] Analysis context created:', {
      confidence: analysisContext.confidence,
      clarificationNeeded: analysisContext.clarificationNeeded
    });
    
    // Check if clarification is needed
    if (analysisContext.clarificationNeeded && analysisContext.confidence < 0.7) {
      const questions = generateClarificationQuestions(imageContext, userContextParsed);
      return {
        requiresClarification: true,
        questions,
        analysisContext
      };
    }
    
    return {
      requiresClarification: false,
      analysisContext
    };
    
  } catch (error) {
    console.error('[ContextDetection] Failed to detect context:', error);
    
    // Create fallback context instead of failing
    const fallbackContext = {
      image: {
        primaryType: 'app',
        domain: 'general',
        complexity: 'moderate',
        platform: 'web'
      },
      user: {
        expertise: 'intermediate',
        inferredRole: null
      },
      confidence: 0.5,
      clarificationNeeded: false
    };
    
    return {
      requiresClarification: false,
      analysisContext: fallbackContext
    };
  }
}

// Helper functions for context detection
function parseImageContext(data: any): any {
  const responseData = data.data || data.result || data;
  
  let primaryType = responseData.primaryType || responseData.interface_type || responseData.type || 'unknown';
  
  if (primaryType === 'unknown' || !primaryType) {
    primaryType = 'app'; // Default fallback
  }
  
  return {
    primaryType,
    subTypes: responseData.subTypes || responseData.sub_types || [],
    domain: responseData.domain || responseData.industry || 'general',
    complexity: responseData.complexity || 'moderate',
    userIntent: responseData.userIntent || responseData.user_intent || [],
    platform: responseData.platform || 'web'
  };
}

function inferUserContext(explicitContext: string): any {
  const context = {
    technicalLevel: 'some-technical',
    expertise: 'intermediate',
    inferredRole: null,
    goals: [],
    focusAreas: []
  };

  // Analyze explicit context for role indicators
  const roleIndicators = {
    designer: /design|ui|ux|visual|aesthetic|color|typography/i,
    developer: /code|component|api|implement|technical|architecture/i,
    business: /revenue|conversion|roi|metrics|growth|acquisition/i,
    product: /feature|roadmap|user story|backlog|priorit/i,
  };

  for (const [role, pattern] of Object.entries(roleIndicators)) {
    if (pattern.test(explicitContext)) {
      context.inferredRole = role;
      break;
    }
  }

  return context;
}

function createAnalysisContext(imageContext: any, userContext: any): any {
  const confidence = calculateContextConfidence(imageContext, userContext);
  
  return {
    image: imageContext,
    user: userContext,
    confidence,
    clarificationNeeded: confidence < 0.7,
    focusAreas: [],
    analysisDepth: 'standard'
  };
}

function calculateContextConfidence(imageContext: any, userContext: any): number {
  let confidence = 0.5; // Base confidence

  // Image context factors
  if (imageContext.primaryType !== 'unknown') confidence += 0.2;
  if (imageContext.domain && imageContext.domain !== 'general') confidence += 0.15;

  // User context factors
  if (userContext.inferredRole) confidence += 0.1;
  if (userContext.goals && userContext.goals.length > 0) confidence += 0.1;

  return Math.min(confidence, 1.0);
}

function generateClarificationQuestions(imageContext: any, userContext: any): string[] {
  const questions: string[] = [];

  // Clarify interface type if uncertain
  if (imageContext.primaryType === 'unknown') {
    questions.push(
      "What type of interface is this? (e.g., dashboard, landing page, mobile app, form, etc.)"
    );
  }

  // Clarify user role if not detected
  if (!userContext.inferredRole) {
    questions.push(
      "What's your role? (designer, developer, product manager, business owner, etc.)"
    );
  }

  // Always have at least one question if confidence is low
  if (questions.length === 0) {
    questions.push(
      "Could you tell me more about this interface and what kind of analysis would be most helpful?"
    );
  }

  return questions;
}

// Safe base64 conversion for large files using chunked approach
async function convertToBase64Chunked(arrayBuffer: ArrayBuffer): Promise<string> {
  try {
    const uint8Array = new Uint8Array(arrayBuffer);
    const chunkSize = 8192; // 8KB chunks to avoid call stack overflow
    let binaryString = '';
    
    console.log(`üìä Converting ${uint8Array.length} bytes to base64 using ${chunkSize} byte chunks`);
    
    for (let i = 0; i < uint8Array.length; i += chunkSize) {
      const chunk = uint8Array.slice(i, i + chunkSize);
      binaryString += String.fromCharCode.apply(null, Array.from(chunk));
      
      // Log progress for very large files
      if (i % (chunkSize * 100) === 0) {
        const progress = Math.round((i / uint8Array.length) * 100);
        console.log(`üìä Base64 conversion progress: ${progress}%`);
      }
    }
    
    console.log('‚úÖ Binary string conversion complete, applying base64 encoding');
    return btoa(binaryString);
  } catch (error) {
    console.error('‚ùå Base64 conversion failed:', error);
    throw new Error(`Base64 conversion failed: ${error.message}`);
  }
}

async function extractGoogleVisionMetadata(imageUrl: string, imageBase64?: string): Promise<any> {
  const googleApiKey = Deno.env.get('GOOGLE_VISION_API_KEY');
  
  if (!googleApiKey) {
    console.warn('‚ö†Ô∏è No Google Vision API key available, using mock metadata');
    return createMockVisionMetadata();
  }
  
  try {
    console.log('üîß Performing Google Vision API call...');
    
    // Convert image to base64 if we have a URL
    let base64Image = imageBase64;
    if (!base64Image && imageUrl) {
      console.log('üì• Fetching image from URL for Vision API call...');
      const imageResponse = await fetch(imageUrl);
      if (!imageResponse.ok) {
        throw new Error(`Failed to fetch image: ${imageResponse.statusText}`);
      }
      const imageBuffer = await imageResponse.arrayBuffer();
      base64Image = await convertToBase64Chunked(imageBuffer);
      console.log('‚úÖ Image converted to base64 for Vision API');
    }
    
    if (!base64Image) {
      throw new Error('No image data available for Vision API call');
    }
    
    // Prepare Google Vision API request
    const visionRequest = {
      requests: [
        {
          image: {
            content: base64Image
          },
          features: [
            { type: 'LABEL_DETECTION', maxResults: 10 },
            { type: 'TEXT_DETECTION', maxResults: 10 },
            { type: 'OBJECT_LOCALIZATION', maxResults: 10 },
            { type: 'FACE_DETECTION', maxResults: 10 },
            { type: 'IMAGE_PROPERTIES', maxResults: 10 }
          ]
        }
      ]
    };
    
    // Call Google Vision API directly
    const response = await fetch(
      `https://vision.googleapis.com/v1/images:annotate?key=${googleApiKey}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(visionRequest)
      }
    );
    
    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Google Vision API error: ${response.status} - ${errorText}`);
    }
    
    const visionResponse = await response.json();
    const result = visionResponse.responses?.[0];
    
    if (!result) {
      throw new Error('No response from Google Vision API');
    }
    
    // Transform Google Vision response to our format
    const transformedMetadata = {
      objects: (result.localizedObjectAnnotations || []).map((obj: any) => ({
        name: obj.name,
        score: obj.score,
        boundingPoly: obj.boundingPoly
      })),
      text: (result.textAnnotations || []).map((text: any) => ({
        description: text.description,
        boundingPoly: text.boundingPoly
      })),
      colors: result.imagePropertiesAnnotation?.dominantColors?.colors || [],
      faces: (result.faceAnnotations || []).length,
      labels: (result.labelAnnotations || []).map((label: any) => ({
        description: label.description,
        score: label.score
      })),
      logos: [],
      confidence: 0.9,
      timestamp: new Date().toISOString()
    };
    
    console.log('‚úÖ Google Vision API call successful');
    return transformedMetadata;
    
  } catch (error) {
    console.error('‚ùå Google Vision API call failed:', error);
    console.warn('‚ö†Ô∏è Falling back to mock metadata');
    return createMockVisionMetadata();
  }
}


function createMockVisionMetadata(): any {
  return {
    objects: [
      { name: 'Button', score: 0.9, boundingPoly: { normalizedVertices: [{ x: 0.1, y: 0.2 }, { x: 0.3, y: 0.4 }] } },
      { name: 'Text', score: 0.95, boundingPoly: { normalizedVertices: [{ x: 0.1, y: 0.1 }, { x: 0.9, y: 0.15 }] } }
    ],
    text: [
      { description: 'Sample UI Text', boundingPoly: { vertices: [{ x: 100, y: 50 }, { x: 200, y: 80 }] } }
    ],
    colors: [
      { color: { red: 255, green: 255, blue: 255 }, score: 0.8, pixelFraction: 0.4 },
      { color: { red: 0, green: 100, blue: 200 }, score: 0.6, pixelFraction: 0.3 }
    ],
    faces: 0,
    labels: [
      { description: 'User interface', score: 0.9 },
      { description: 'Software', score: 0.85 }
    ],
    logos: [],
    confidence: 0.75,
    timestamp: new Date().toISOString()
  };
}

async function runOpenAIAnalysis(imageUrl: string, imageBase64?: string, userContext?: string, visionMetadata?: any, enhancedContext?: any): Promise<any> {
  console.log('ü§ñ Running OpenAI analysis with vision metadata...');
  
  // Use enhanced prompt if available, otherwise build standard prompt
  const enhancedPrompt = enhancedContext?.contextualPrompt || buildEnhancedAnalysisPrompt(userContext, visionMetadata, 'openai');
  
  const payload = {
    model: 'gpt-4o',
    stage: 'analysis',
    imageUrl,
    imageBase64,
    prompt: enhancedPrompt,
    systemPrompt: `You are a senior UX/UI designer with deep expertise in visual design, usability, and design systems. 
    You have access to Google Vision metadata about this interface including detected objects, text, colors, and layout elements.
    Use this metadata to provide more accurate and detailed analysis.
    Always provide your responses in valid JSON format as specified in the user prompt.`
  };
  
  console.log('üîç OPENAI DEBUG - Sending payload:', {
    model: payload.model,
    stage: payload.stage,
    promptLength: payload.prompt?.length,
    hasImage: !!(imageUrl || imageBase64)
  });
  
  const response = await executeOpenAI(MODEL_CONFIGS['gpt-4o'], Deno.env.get('OPENAI_API_KEY')!, payload);
  const responseText = await response.text();
  
  console.log('üîç OPENAI DEBUG - Raw response received:', {
    length: responseText.length,
    firstChars: responseText.substring(0, 200) + '...',
    containsSuggestions: responseText.includes('suggestions'),
    containsAnnotations: responseText.includes('visualAnnotations') || responseText.includes('visual_annotations')
  });
  
  let result;
  try {
    result = JSON.parse(responseText);
    console.log('üîç OPENAI DEBUG - Parsed result structure:', {
      hasSuggestions: !!result.suggestions,
      suggestionsCount: result.suggestions?.length || 0,
      hasVisualAnnotations: !!(result.visualAnnotations || result.visual_annotations),
      annotationsCount: (result.visualAnnotations || result.visual_annotations || []).length,
      hasSummary: !!result.summary,
      topLevelKeys: Object.keys(result)
    });
  } catch (parseError) {
    console.error('‚ùå OPENAI DEBUG - JSON parse failed:', parseError);
    console.log('Raw response that failed to parse:', responseText);
    throw new Error(`OpenAI response JSON parse failed: ${parseError.message}`);
  }
  
  console.log('‚úÖ OpenAI analysis complete');
  return { model: 'gpt-4o', result, confidence: 0.9, imageUrl };
}

async function runClaudeAnalysis(imageUrl: string, imageBase64?: string, userContext?: string, visionMetadata?: any, enhancedContext?: any): Promise<any> {
  console.log('ü§ñ Running Claude analysis with vision metadata...');
  
  // Use enhanced prompt if available, otherwise build standard prompt
  const enhancedPrompt = enhancedContext?.contextualPrompt || buildEnhancedAnalysisPrompt(userContext, visionMetadata, 'claude');
  
  const payload = {
    model: 'claude-opus-4-20250514',
    stage: 'analysis',
    imageUrl,
    imageBase64,
    prompt: enhancedPrompt,
    systemPrompt: `You are a senior UX/UI designer with deep expertise in visual design, usability, and design systems.
    You have access to Google Vision metadata about this interface including detected objects, text, colors, and layout elements.
    Use this metadata to provide more accurate and detailed analysis.
    Return your analysis as structured JSON as specified in the user prompt.`
  };
  
  console.log('üîç CLAUDE DEBUG - Sending payload:', {
    model: payload.model,
    stage: payload.stage,
    promptLength: payload.prompt?.length,
    hasImage: !!(imageUrl || imageBase64)
  });
  
  const response = await executeAnthropic(MODEL_CONFIGS['claude-opus-4-20250514'], Deno.env.get('ANTHROPIC_API_KEY')!, payload);
  const responseText = await response.text();
  
  console.log('üîç CLAUDE DEBUG - Raw response received:', {
    length: responseText.length,
    firstChars: responseText.substring(0, 200) + '...',
    containsSuggestions: responseText.includes('suggestions'),
    containsAnnotations: responseText.includes('visualAnnotations') || responseText.includes('visual_annotations')
  });
  
  let result;
  try {
    result = JSON.parse(responseText);
    console.log('üîç CLAUDE DEBUG - Parsed result structure:', {
      hasSuggestions: !!result.suggestions,
      suggestionsCount: result.suggestions?.length || 0,
      hasVisualAnnotations: !!(result.visualAnnotations || result.visual_annotations),
      annotationsCount: (result.visualAnnotations || result.visual_annotations || []).length,
      hasSummary: !!result.summary,
      topLevelKeys: Object.keys(result)
    });
  } catch (parseError) {
    console.error('‚ùå CLAUDE DEBUG - JSON parse failed:', parseError);
    console.log('Raw response that failed to parse:', responseText);
    throw new Error(`Claude response JSON parse failed: ${parseError.message}`);
  }
  
  console.log('‚úÖ Claude analysis complete');
  return { model: 'claude-opus-4-20250514', result, confidence: 0.88, imageUrl };
}

function buildEnhancedAnalysisPrompt(userContext?: string, visionMetadata?: any, targetModel?: string): string {
  const contextPart = userContext ? `User Context: ${userContext}` : 'General UX analysis';
  
  const visionContext = visionMetadata ? `
Google Vision Metadata Available:
- Detected Objects: ${visionMetadata.objects?.length || 0} UI elements
- Text Elements: ${visionMetadata.text?.length || 0} text blocks
- Color Palette: ${visionMetadata.colors?.length || 0} dominant colors
- Faces Detected: ${visionMetadata.faces || 0}
- UI Labels: ${visionMetadata.labels?.map(l => l.description).join(', ') || 'None'}

Use this metadata to enhance your analysis accuracy and provide specific coordinate-based annotations.
` : '';

  // Critical: Different models need different instruction styles for JSON output
  const jsonInstruction = targetModel === 'claude' ? 
    `You MUST respond with ONLY valid JSON. No markdown, no explanations, JUST the JSON object.` :
    `Return your response as a valid JSON object following the exact structure specified below.`;

  return `${jsonInstruction}

CRITICAL INSTRUCTION: You MUST provide a complete UX analysis with ALL required fields populated. Do not return empty arrays or placeholder data.

Analyze this application interface for usability, navigation patterns, and feature discoverability using the provided Google Vision metadata.

${visionContext}

MANDATORY ANALYSIS REQUIREMENTS:
- You MUST provide AT LEAST 3-5 specific suggestions in the suggestions array
- You MUST provide AT LEAST 2-4 visual annotations with specific coordinates
- You MUST calculate actual scores based on what you observe in the interface
- You MUST identify real issues and improvements, not generic placeholders

Perform comprehensive UX analysis focusing on:

1. **Visual Hierarchy & Layout**
   - Use detected object coordinates for precise annotations
   - Evaluate spacing, alignment, and visual flow
   - Assess typography scale and readability

2. **Interaction Design**
   - Analyze button placement and sizing
   - Review form field organization
   - Evaluate navigation patterns

3. **Accessibility & Standards**
   - Check WCAG-AA compliance
   - Evaluate color contrast using detected colors
   - Review text readability and sizing

4. **Visual Design Quality**
   - Assess color harmony using vision metadata
   - Evaluate design consistency
   - Review brand expression

REQUIRED JSON STRUCTURE (you must populate ALL fields with real data):
{
  "visualAnnotations": [
    {
      "id": "annotation_1",
      "x": 0.1,
      "y": 0.2,
      "type": "issue",
      "title": "Specific Issue Title (NOT generic)",
      "description": "Detailed description of actual observation",
      "severity": "medium",
      "category": "usability"
    }
  ],
  "suggestions": [
    {
      "id": "suggestion_1",
      "category": "usability",
      "title": "Actionable recommendation (NOT placeholder)",
      "description": "Detailed explanation based on actual interface analysis",
      "impact": "medium",
      "effort": "medium",
      "actionItems": ["Specific step 1", "Specific step 2"]
    }
  ],
  "summary": {
    "overallScore": 85,
    "categoryScores": {
      "usability": 80,
      "accessibility": 75,
      "visual": 90,
      "content": 85
    },
    "keyIssues": ["Actual issue 1", "Actual issue 2"],
    "strengths": ["Real strength 1", "Real strength 2"]
  },
  "metadata": {
    "detectedElements": 12,
    "primaryColors": ["#FFFFFF", "#0066CC"],
    "confidenceScore": 0.92
  }
}

VALIDATION CHECKLIST (verify before responding):
‚úì suggestions array has at least 3 items with real recommendations
‚úì visualAnnotations array has at least 2 items with specific coordinates
‚úì summary contains calculated scores based on interface analysis
‚úì All descriptions are specific to this interface, not generic
‚úì actionItems contain concrete, implementable steps
‚úì keyIssues and strengths reflect actual observations

${contextPart}

FINAL REMINDER: Respond with ONLY the JSON object. No explanations, no markdown formatting, just valid JSON.`;
}

async function synthesizeMultiModelResults(
  openaiAnalysis: any,
  claudeAnalysis: any,
  visionMetadata: any,
  context: { imageName?: string; imageId?: string; userContext?: string }
): Promise<any> {
  console.log('üîÆ Synthesizing results from OpenAI, Claude, and Google Vision...');
  
  try {
    // Debug input data structure with detailed logging
    console.log('üîç SYNTHESIS DEBUG - Raw input structure:', {
      openaiAnalysis: openaiAnalysis ? {
        hasResult: !!openaiAnalysis.result,
        resultKeys: openaiAnalysis.result ? Object.keys(openaiAnalysis.result) : [],
        model: openaiAnalysis.model,
        confidence: openaiAnalysis.confidence
      } : null,
      claudeAnalysis: claudeAnalysis ? {
        hasResult: !!claudeAnalysis.result,
        resultKeys: claudeAnalysis.result ? Object.keys(claudeAnalysis.result) : [],
        model: claudeAnalysis.model,
        confidence: claudeAnalysis.confidence
      } : null,
      visionMetadata: visionMetadata ? Object.keys(visionMetadata) : null
    });
    
    // CRITICAL FIX: Handle different response formats and field names
    const extractDataFromResult = (analysisResult: any) => {
      if (!analysisResult?.result) {
        console.log('üîç SYNTHESIS DEBUG - No result found in analysis:', analysisResult);
        return { suggestions: [], visualAnnotations: [], summary: null };
      }
      
      const result = analysisResult.result;
      console.log('üîç SYNTHESIS DEBUG - Raw result keys:', Object.keys(result));
      
      // Handle different field naming conventions and nested structures
      let suggestions = result.suggestions || result.recommendations || [];
      let visualAnnotations = result.visualAnnotations || result.visual_annotations || result.annotations || [];
      let summary = result.summary || result.executiveSummary || null;
      
      // CRITICAL: Handle domain-specific analysis results that don't follow standard structure
      // If we get domain-specific keys like 'data_visualization_clarity', try to convert them
      if (suggestions.length === 0 && visualAnnotations.length === 0) {
        console.log('üîç SYNTHESIS DEBUG - Standard fields empty, checking for domain-specific content');
        
        // Try to extract from domain-specific analysis
        const domainKeys = Object.keys(result).filter(key => 
          !['metadata', 'summary', 'confidence'].includes(key)
        );
        
        if (domainKeys.length > 0) {
          console.log('üîç SYNTHESIS DEBUG - Found domain-specific keys:', domainKeys);
          
          // Convert domain analysis to standard format
          const convertedSuggestions = [];
          const convertedAnnotations = [];
          
          domainKeys.forEach((key, index) => {
            const domainData = result[key];
            if (domainData && typeof domainData === 'object') {
              // Create suggestion from domain analysis using Smart Text Formatter
              const smartDescription = SmartTextFormatter.formatAnalysisDescription(domainData);
              convertedSuggestions.push({
                id: `converted_${index + 1}`,
                category: 'usability',
                title: `${key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase())} Improvement`,
                description: smartDescription || SmartTextFormatter.formatFallback(domainData),
                impact: 'medium',
                effort: 'medium',
                actionItems: ['Review analysis findings', 'Implement recommended changes']
              });
              
              // Create annotation from domain analysis with improved context
              const annotationDescription = generateContextualAnnotationDescription(key, domainData);
              convertedAnnotations.push({
                id: `converted_ann_${index + 1}`,
                x: 0.5,
                y: 0.3 + (index * 0.1),
                type: 'suggestion',
                title: `${key.replace(/_/g, ' ')} Area`,
                description: annotationDescription,
                severity: 'medium',
                category: 'usability'
              });
            }
          });
          
          if (convertedSuggestions.length > 0) {
            suggestions = convertedSuggestions;
            visualAnnotations = convertedAnnotations;
            console.log('üîç SYNTHESIS DEBUG - Converted domain analysis to standard format');
          }
        }
      }
      
      console.log('üîç SYNTHESIS DEBUG - Extracted from result:', {
        model: analysisResult.model,
        suggestionsCount: suggestions.length,
        annotationsCount: visualAnnotations.length,
        hasSummary: !!summary,
        originalKeys: Object.keys(result),
        converted: suggestions.length > 0 && suggestions[0]?.id?.startsWith('converted')
      });
      
      return { suggestions, visualAnnotations, summary };
    };
    
    // Extract data from both models
    const openaiData = extractDataFromResult(openaiAnalysis);
    const claudeData = extractDataFromResult(claudeAnalysis);
    
    // Combine and deduplicate visual annotations
    const allAnnotations = [...openaiData.visualAnnotations, ...claudeData.visualAnnotations];
    
    // Combine and rank suggestions by consensus
    const allSuggestions = [...openaiData.suggestions, ...claudeData.suggestions];
    
    console.log('üîç SYNTHESIS DEBUG - Combined data:', {
      totalAnnotations: allAnnotations.length,
      totalSuggestions: allSuggestions.length,
      openaiContributions: {
        suggestions: openaiData.suggestions.length,
        annotations: openaiData.visualAnnotations.length
      },
      claudeContributions: {
        suggestions: claudeData.suggestions.length,
        annotations: claudeData.visualAnnotations.length
      }
    });
    
    // If we have no analysis data, create fallback content
    if (allSuggestions.length === 0 && allAnnotations.length === 0) {
      console.warn('‚ö†Ô∏è SYNTHESIS WARNING - No analysis data found, creating fallback content');
      
      // Create minimal fallback analysis based on vision metadata
      const fallbackSuggestions = [{
        id: 'fallback_1',
        category: 'usability',
        title: 'Interface Analysis Required',
        description: 'This interface needs detailed analysis. Consider reviewing navigation patterns, information hierarchy, and user interaction flows.',
        impact: 'medium',
        effort: 'medium',
        actionItems: ['Review navigation structure', 'Analyze content organization', 'Test user workflows']
      }];
      
      const fallbackAnnotations = this.generateContextualFallbackAnnotations(userContext, interfaceHints);
      
      return {
        id: `analysis_${Date.now()}`,
        imageId: context.imageId || '',
        imageName: context.imageName || 'Untitled Image',
        imageUrl: openaiAnalysis?.imageUrl || claudeAnalysis?.imageUrl || '',
        userContext: context.userContext || '',
        visualAnnotations: fallbackAnnotations,
        suggestions: fallbackSuggestions,
        summary: this.generateContextualFallbackSummary(userContext, interfaceHints),
        metadata: {
          ...(visionMetadata || {}),
          modelsUsed: ['gpt-4o', 'claude-opus-4-20250514', 'google-vision-service'],
          fallbackUsed: true,
          reason: 'No analysis data extracted from AI responses',
          aiGenerated: true,
          analysisTimestamp: new Date().toISOString(),
          pipelineVersion: '3.0-multi-model-fallback'
        },
        createdAt: new Date(),
        modelUsed: 'multi-model-synthesis-fallback',
        status: 'completed'
      };
    }
    
    // Calculate consensus scores safely
    const openaiSummary = openaiData.summary || {};
    const claudeSummary = claudeData.summary || {};
    
    const synthesizedSummary = {
      overallScore: Math.round(((openaiSummary.overallScore || 75) + (claudeSummary.overallScore || 75)) / 2),
      categoryScores: {
        usability: Math.round(((openaiSummary.categoryScores?.usability || 75) + (claudeSummary.categoryScores?.usability || 75)) / 2),
        accessibility: Math.round(((openaiSummary.categoryScores?.accessibility || 75) + (claudeSummary.categoryScores?.accessibility || 75)) / 2),
        visual: Math.round(((openaiSummary.categoryScores?.visual || 75) + (claudeSummary.categoryScores?.visual || 75)) / 2),
        content: Math.round(((openaiSummary.categoryScores?.content || 75) + (claudeSummary.categoryScores?.content || 75)) / 2)
      },
      keyIssues: [
        ...(openaiSummary.keyIssues || []),
        ...(claudeSummary.keyIssues || [])
      ].slice(0, 5), // Top 5 issues
      strengths: [
        ...(openaiSummary.strengths || []),
        ...(claudeSummary.strengths || [])
      ].slice(0, 5) // Top 5 strengths
    };
    
    // Enhanced metadata with multi-model consensus
    const enhancedMetadata = {
      ...(visionMetadata || {}),
      modelsUsed: ['gpt-4o', 'claude-opus-4-20250514', 'google-vision-service'],
      consensus: {
        openaiConfidence: openaiAnalysis?.confidence || 0.8,
        claudeConfidence: claudeAnalysis?.confidence || 0.8,
        visionConfidence: visionMetadata?.confidence || 0.8,
        overallConfidence: ((openaiAnalysis?.confidence || 0.8) + (claudeAnalysis?.confidence || 0.8) + (visionMetadata?.confidence || 0.8)) / 3
      },
      analysisTimestamp: new Date().toISOString(),
      pipelineVersion: '3.0-multi-model',
      dataExtracted: {
        openaiContributed: openaiData.suggestions.length + openaiData.visualAnnotations.length,
        claudeContributed: claudeData.suggestions.length + claudeData.visualAnnotations.length
      },
      aiGenerated: true // CRITICAL: Mark as real AI analysis
    };
    
    console.log('‚úÖ Multi-model synthesis complete');
    
    const finalResult = {
      id: `analysis_${Date.now()}`,
      imageId: context.imageId || '',
      imageName: context.imageName || 'Untitled Image',
      imageUrl: openaiAnalysis?.imageUrl || claudeAnalysis?.imageUrl || '',
      userContext: context.userContext || '',
      visualAnnotations: allAnnotations.slice(0, 10), // Top 10 annotations
      suggestions: allSuggestions.slice(0, 8), // Top 8 suggestions
      summary: synthesizedSummary,
      metadata: enhancedMetadata,
      createdAt: new Date(),
      modelUsed: 'multi-model-synthesis',
      status: 'completed'
    };
    
    console.log('üîç SYNTHESIS FINAL - Result structure:', {
      id: finalResult.id,
      visualAnnotationsCount: finalResult.visualAnnotations.length,
      suggestionsCount: finalResult.suggestions.length,
      hasSummary: !!finalResult.summary,
      overallScore: finalResult.summary?.overallScore,
      metadataKeys: Object.keys(finalResult.metadata)
    });
    
    // Validate final result before returning
    if (finalResult.suggestions.length === 0 && finalResult.visualAnnotations.length === 0) {
      console.error('‚ùå SYNTHESIS ERROR - Final result has no content despite synthesis attempt');
      console.log('Debug final result:', JSON.stringify(finalResult, null, 2));
    }
    
    return finalResult;
    
  } catch (error) {
    console.error('‚ùå Error in multi-model synthesis:', error);
    console.error('Synthesis error stack:', error.stack);
    
    // Enhanced fallback with better error handling
    console.log('üîÑ Creating enhanced fallback result...');
    
    const primaryResult = openaiAnalysis?.result || claudeAnalysis?.result;
    const fallbackResult = primaryResult || {};
    
    return {
      id: `analysis_${Date.now()}`,
      imageId: context.imageId || '',
      imageName: context.imageName || 'Untitled Image',
      imageUrl: openaiAnalysis?.imageUrl || claudeAnalysis?.imageUrl || '',
      userContext: context.userContext || '',
      visualAnnotations: fallbackResult.visualAnnotations || fallbackResult.visual_annotations || [{
        id: 'error_fallback_1',
        x: 0.5,
        y: 0.5,
        type: 'issue',
        title: 'Synthesis Error',
        description: 'Analysis synthesis encountered an error. Manual review recommended.',
        severity: 'medium',
        category: 'usability'
      }],
      suggestions: fallbackResult.suggestions || fallbackResult.recommendations || [{
        id: 'error_fallback_suggestion_1',
        category: 'usability',
        title: 'Analysis Error Recovery',
        description: 'The analysis pipeline encountered an error during synthesis. Please try the analysis again.',
        impact: 'medium',
        effort: 'low',
        actionItems: ['Retry analysis', 'Check image quality', 'Verify connectivity']
      }],
      summary: fallbackResult.summary || {
        overallScore: 70,
        categoryScores: { usability: 70, accessibility: 70, visual: 70, content: 70 },
        keyIssues: ['Synthesis error occurred'],
        strengths: []
      },
      metadata: {
        ...visionMetadata,
        fallbackUsed: true,
        synthesisError: error.message,
        errorStack: error.stack,
        originalDataAvailable: !!(openaiAnalysis || claudeAnalysis),
        aiGenerated: true,
        analysisTimestamp: new Date().toISOString(),
        pipelineVersion: '3.0-multi-model-error-fallback'
      },
      createdAt: new Date(),
      modelUsed: 'error-fallback-synthesis',
      status: 'completed_with_errors'
    };
  }
}

// Enhanced context-aware fallback generation functions
function inferInterfaceFromContext(userContext: string, visionMetadata: any): any {
  const lowerContext = userContext.toLowerCase();
  
  // Check vision metadata for interface clues
  const labels = visionMetadata?.labels?.map((l: any) => l.description.toLowerCase()) || [];
  const objects = visionMetadata?.objects?.map((o: any) => o.name.toLowerCase()) || [];
  const allElements = [...labels, ...objects];
  
  const interfaceHints = {
    type: 'app',
    domain: 'general',
    complexity: 'moderate',
    elements: allElements
  };
  
  // Infer interface type
  if (lowerContext.includes('dashboard') || allElements.some(el => ['chart', 'graph', 'table'].includes(el))) {
    interfaceHints.type = 'dashboard';
    interfaceHints.domain = 'analytics';
  } else if (lowerContext.includes('landing') || lowerContext.includes('marketing')) {
    interfaceHints.type = 'landing';
    interfaceHints.domain = 'marketing';
  } else if (lowerContext.includes('mobile') || allElements.some(el => ['mobile', 'app'].includes(el))) {
    interfaceHints.type = 'mobile';
    interfaceHints.domain = 'technology';
  } else if (lowerContext.includes('ecommerce') || allElements.some(el => ['product', 'cart', 'shop'].includes(el))) {
    interfaceHints.type = 'ecommerce';
    interfaceHints.domain = 'retail';
  }
  
  // Infer complexity from element count and context
  if (allElements.length > 15 || lowerContext.includes('complex') || lowerContext.includes('enterprise')) {
    interfaceHints.complexity = 'complex';
  } else if (allElements.length < 5 || lowerContext.includes('simple') || lowerContext.includes('minimal')) {
    interfaceHints.complexity = 'simple';
  }
  
  return interfaceHints;
}

function generateContextualFallbackSuggestions(userContext: string, interfaceHints: any): any[] {
  const suggestions = [];
  const lowerContext = userContext.toLowerCase();
  
  // Generate suggestions based on interface type
  switch (interfaceHints.type) {
    case 'dashboard':
      suggestions.push({
        id: 'fallback_dashboard_1',
        category: 'usability',
        title: 'Optimize Data Hierarchy',
        description: 'Review the arrangement of charts and metrics to ensure the most important data is prominently displayed and easily accessible.',
        impact: 'high',
        effort: 'medium',
        actionItems: ['Prioritize key metrics at the top', 'Group related data visualizations', 'Use consistent spacing and sizing']
      });
      break;
      
    case 'landing':
      suggestions.push({
        id: 'fallback_landing_1',
        category: 'conversion',
        title: 'Enhance Call-to-Action Visibility',
        description: 'Improve the prominence and clarity of primary conversion elements to guide user action effectively.',
        impact: 'high',
        effort: 'low',
        actionItems: ['Make primary CTA button more prominent', 'Clarify value proposition', 'Reduce visual distractions']
      });
      break;
      
    case 'mobile':
      suggestions.push({
        id: 'fallback_mobile_1',
        category: 'usability',
        title: 'Optimize Touch Targets',
        description: 'Ensure all interactive elements meet minimum touch target size requirements for mobile accessibility.',
        impact: 'high',
        effort: 'low',
        actionItems: ['Increase button sizes to 44px minimum', 'Add appropriate spacing between elements', 'Test on various device sizes']
      });
      break;
      
    case 'ecommerce':
      suggestions.push({
        id: 'fallback_ecommerce_1',
        category: 'conversion',
        title: 'Streamline Product Discovery',
        description: 'Improve product navigation and search functionality to help users find desired items more efficiently.',
        impact: 'high',
        effort: 'medium',
        actionItems: ['Enhance search filters', 'Improve product categorization', 'Add visual product previews']
      });
      break;
      
    default:
      suggestions.push({
        id: 'fallback_general_1',
        category: 'usability',
        title: 'Improve Information Architecture',
        description: 'Optimize the organization and presentation of content to enhance user understanding and navigation.',
        impact: 'medium',
        effort: 'medium',
        actionItems: ['Review content hierarchy', 'Simplify navigation structure', 'Improve visual consistency']
      });
  }
  
  // Add context-specific suggestions based on user input
  if (lowerContext.includes('accessibility') || lowerContext.includes('a11y')) {
    suggestions.push({
      id: 'fallback_accessibility_1',
      category: 'accessibility',
      title: 'Enhance Accessibility Compliance',
      description: 'Improve interface accessibility to meet WCAG guidelines and serve users with disabilities.',
      impact: 'high',
      effort: 'medium',
      actionItems: ['Add alt text to images', 'Improve color contrast ratios', 'Ensure keyboard navigation support']
    });
  }
  
  if (lowerContext.includes('conversion') || lowerContext.includes('business')) {
    suggestions.push({
      id: 'fallback_conversion_1',
      category: 'conversion',
      title: 'Optimize Conversion Funnel',
      description: 'Identify and remove friction points in the user journey to improve conversion rates.',
      impact: 'high',
      effort: 'medium',
      actionItems: ['Simplify form fields', 'Add trust signals', 'Clarify value proposition']
    });
  }
  
  return suggestions.slice(0, 5); // Return top 5 suggestions
}

function generateContextualFallbackAnnotations(userContext: string, interfaceHints: any): any[] {
  const annotations = [];
  
  // Generate annotations based on interface type and complexity
  if (interfaceHints.type === 'dashboard') {
    annotations.push({
      id: 'fallback_ann_dashboard_1',
      x: 0.2,
      y: 0.1,
      type: 'suggestion',
      title: 'Key Metrics Area',
      description: 'Primary dashboard area for displaying critical performance indicators',
      severity: 'medium',
      category: 'usability'
    });
  } else if (interfaceHints.type === 'landing') {
    annotations.push({
      id: 'fallback_ann_landing_1',
      x: 0.5,
      y: 0.3,
      type: 'suggestion',
      title: 'Hero Section',
      description: 'Main conversion area requiring clear value proposition and call-to-action',
      severity: 'medium',
      category: 'conversion'
    });
  } else if (interfaceHints.type === 'mobile') {
    annotations.push({
      id: 'fallback_ann_mobile_1',
      x: 0.5,
      y: 0.9,
      type: 'suggestion',
      title: 'Navigation Area',
      description: 'Mobile navigation requires optimization for thumb-friendly interaction',
      severity: 'medium',
      category: 'usability'
    });
  }
  
  // Always add a general usability annotation
  annotations.push({
    id: 'fallback_ann_general_1',
    x: 0.5,
    y: 0.5,
    type: 'info',
    title: 'Interface Review Area',
    description: 'Central interface area that would benefit from detailed UX analysis',
    severity: 'low',
    category: 'usability'
  });
  
  return annotations.slice(0, 3); // Return top 3 annotations
}

function generateContextualAnnotationDescription(key: string, domainData: any): string {
  // Generate contextual descriptions for annotations based on domain analysis key
  const keyMappings: Record<string, string> = {
    cart_and_checkout_flow_optimization: 'Shopping cart and checkout process requires optimization for better conversion rates and user experience.',
    product_presentation: 'Product display and presentation elements need enhancement to improve user engagement and sales conversion.',
    navigation_usability: 'Navigation structure and usability improvements needed to enhance user flow and findability.',
    mobile_responsiveness: 'Mobile responsive design elements require attention for optimal cross-device experience.',
    accessibility_compliance: 'Accessibility features and compliance standards need improvement for inclusive user experience.',
    page_loading_performance: 'Page loading speed and performance optimization needed to reduce bounce rates.',
    visual_hierarchy: 'Visual hierarchy and information organization require restructuring for better user comprehension.',
    trust_signals: 'Trust indicators and security signals need enhancement to improve user confidence.',
    call_to_action_optimization: 'Call-to-action elements require optimization for better visibility and conversion rates.',
    form_usability: 'Form design and usability improvements needed to reduce abandonment and increase completion rates.'
  };

  // Use predefined mapping or generate from key
  const description = keyMappings[key] || 
    `${key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase())} area identified for improvement based on analysis findings.`;

  return description;
}

function generateContextualFallbackSummary(userContext: string, interfaceHints: any): any {
  const baseScore = interfaceHints.complexity === 'simple' ? 80 : 
                   interfaceHints.complexity === 'complex' ? 65 : 75;
  
  // Adjust scores based on interface type
  const typeAdjustments: Record<string, any> = {
    dashboard: { usability: -5, visual: +10, accessibility: -10 },
    landing: { usability: +5, visual: +5, accessibility: +5 },
    mobile: { usability: +10, visual: 0, accessibility: +5 },
    ecommerce: { usability: 0, visual: +5, accessibility: 0 }
  };
  
  const adjustments = typeAdjustments[interfaceHints.type] || { usability: 0, visual: 0, accessibility: 0 };
  
  const keyIssues = [];
  const strengths = [];
  
  // Generate contextual issues and strengths
  if (interfaceHints.complexity === 'complex') {
    keyIssues.push('Interface complexity may impact user comprehension');
    strengths.push('Feature-rich interface offers comprehensive functionality');
  } else if (interfaceHints.complexity === 'simple') {
    strengths.push('Clean, minimal design promotes easy navigation');
    keyIssues.push('May benefit from additional functionality or content');
  }
  
  // Add interface-specific issues
  switch (interfaceHints.type) {
    case 'dashboard':
      keyIssues.push('Data visualization hierarchy needs optimization');
      strengths.push('Information-dense interface for data analysis');
      break;
    case 'landing':
      keyIssues.push('Conversion elements require enhanced visibility');
      strengths.push('Marketing-focused design with clear intent');
      break;
    case 'mobile':
      keyIssues.push('Touch targets may need size optimization');
      strengths.push('Mobile-optimized interface design');
      break;
    case 'ecommerce':
      keyIssues.push('Product discovery flow needs streamlining');
      strengths.push('Commerce-focused user experience');
      break;
  }
  
  // Add fallback notice
  keyIssues.push('Complete analysis requires detailed review');
  
  return {
    overallScore: Math.max(60, Math.min(85, baseScore)),
    categoryScores: {
      usability: Math.max(60, Math.min(90, baseScore + adjustments.usability)),
      accessibility: Math.max(60, Math.min(90, baseScore + adjustments.accessibility - 5)), // Generally lower for fallback
      visual: Math.max(60, Math.min(90, baseScore + adjustments.visual)),
      content: Math.max(60, Math.min(90, baseScore))
    },
    keyIssues: keyIssues.slice(0, 4),
    strengths: strengths.slice(0, 3)
  };
}

// CRITICAL FIX: Enhanced database persistence function
async function saveAnalysisToDatabase(imageId: string, analysisData: any) {
  console.log('üíæ Saving analysis to ux_analyses table for image:', imageId);
  console.log('üíæ Analysis data structure:', {
    hasSuggestions: !!analysisData.suggestions,
    suggestionsCount: analysisData.suggestions?.length || 0,
    hasAnnotations: !!analysisData.visualAnnotations,
    annotationsCount: analysisData.visualAnnotations?.length || 0,
    hasSummary: !!analysisData.summary,
    hasMetadata: !!analysisData.metadata
  });
  
  // Create Supabase client for database operations
  const supabaseClient = createClient(
    Deno.env.get('SUPABASE_URL')!,
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!,
    {
      auth: {
        autoRefreshToken: false,
        persistSession: false
      }
    }
  );

  try {
    // Get the project_id for this image to ensure proper association
    const { data: imageData, error: imageError } = await supabaseClient
      .from('images')
      .select('project_id')
      .eq('id', imageId)
      .single();

    if (imageError || !imageData) {
      console.error('Failed to get image project_id:', imageError);
      throw new Error(`Failed to get project for image ${imageId}`);
    }

    const projectId = imageData.project_id;
    console.log('Found project_id for image:', projectId);

    // Prepare data for database with validation
    const visualAnnotations = analysisData.visualAnnotations || analysisData.visual_annotations || [];
    const suggestions = analysisData.suggestions || analysisData.recommendations || [];
    const summary = analysisData.summary || {};
    const metadata = {
      ...analysisData.metadata,
      aiGenerated: true,
      savedAt: new Date().toISOString(),
      dataValidation: {
        suggestionsProvided: suggestions.length,
        annotationsProvided: visualAnnotations.length,
        summaryProvided: !!summary.overallScore
      }
    };

    console.log('üíæ Saving to database:', {
      imageId,
      projectId,
      visualAnnotationsCount: visualAnnotations.length,
      suggestionsCount: suggestions.length,
      hasSummary: !!summary.overallScore
    });

    // Save analysis to ux_analyses table
    const { data, error } = await supabaseClient
      .from('ux_analyses')
      .insert({
        image_id: imageId,
        project_id: projectId,
        visual_annotations: visualAnnotations,
        suggestions: suggestions,
        summary: summary,
        metadata: metadata,
        user_context: analysisData.userContext || '',
        analysis_type: 'full_analysis',
        status: analysisData.status || 'completed'
      })
      .select()
      .single();

    if (error) {
      console.error('‚ùå Database save error:', error);
      console.error('‚ùå Data that failed to save:', {
        visual_annotations: visualAnnotations,
        suggestions: suggestions,
        summary: summary
      });
      throw error;
    }

    console.log('‚úÖ Analysis saved successfully with ID:', data.id);
    console.log('‚úÖ Saved data verification:', {
      savedId: data.id,
      savedSuggestionsCount: data.suggestions?.length || 0,
      savedAnnotationsCount: data.visual_annotations?.length || 0,
      savedSummaryScore: data.summary?.overallScore
    });
    
    // Also save Vision metadata to the images table if available
    if (analysisData.visionMetadata || analysisData.metadata?.vision) {
      await saveVisionMetadataToImage(supabaseClient, imageId, analysisData.visionMetadata || analysisData.metadata?.vision);
    }
    
    return data;

  } catch (error) {
    console.error('‚ùå Failed to save analysis to database:', error);
    console.error('‚ùå Failed analysis data:', JSON.stringify(analysisData, null, 2));
    throw error;
  }
}

// Save Vision metadata to the images table
async function saveVisionMetadataToImage(supabaseClient: any, imageId: string, visionMetadata: any) {
  try {
    console.log('üíæ Saving Vision metadata to images table for image:', imageId);
    
    const { error } = await supabaseClient
      .from('images')
      .update({
        metadata: {
          vision: visionMetadata,
          lastAnalyzed: new Date().toISOString(),
          source: 'google-vision-api'
        }
      })
      .eq('id', imageId);

    if (error) {
      console.error('Failed to save Vision metadata to images table:', error);
    } else {
      console.log('‚úÖ Vision metadata saved to images table');
    }
  } catch (error) {
    console.error('Error saving Vision metadata to images table:', error);
  }
}

// Enhanced Context Analysis Handler
async function handleEnhancedContextAnalysis(body: any) {
  console.log('üîç Enhanced Context Analysis - Processing request:', {
    hasPayload: !!body.payload,
    hasAnalysisContext: !!(body.payload?.analysisContext || body.analysisContext),
    hasMetadata: !!(body.payload?.metadata || body.metadata),
    hasPrompt: !!(body.payload?.prompt || body.prompt),
    imageId: body.payload?.imageId || body.imageId,
    imageName: body.payload?.imageName || body.imageName
  });

  try {
    // PHASE 1: Fix data extraction to handle body.payload structure
    const payload = body.payload || body;
    const { 
      imageId, 
      imageUrl, 
      imageName, 
      userContext,
      analysisContext,
      metadata,
      prompt 
    } = payload;

    // Validate required fields
    if (!imageId || !imageUrl) {
      throw new Error('Missing required fields: imageId and imageUrl are required');
    }

    console.log('üéØ Context detected:', {
      interfaceType: analysisContext?.image?.primaryType,
      domain: analysisContext?.image?.domain,
      userRole: analysisContext?.user?.inferredRole,
      confidence: analysisContext?.confidence
    });

    // Use the existing multi-model pipeline with enhanced context
    const pipelineResult = await executeMultiModelPipeline({
      imageId,
      imageUrl,
      imageName: imageName || 'Enhanced Analysis',
      userContext: userContext || '',
      enhancedContext: {
        analysisContext,
        metadata,
        contextualPrompt: prompt
      }
    });

    // Enhance the results with context information
    const enhancedResult = {
      ...pipelineResult,
      analysisContext,
      contextMetadata: metadata,
      enhancedPrompt: prompt,
      contextualInsights: {
        interfaceType: analysisContext?.image?.primaryType,
        detectedDomain: analysisContext?.image?.domain,
        adaptedForRole: analysisContext?.user?.inferredRole,
        confidenceScore: analysisContext?.confidence,
        clarificationProvided: !analysisContext?.clarificationNeeded
      }
    };

    console.log('‚úÖ Enhanced context analysis completed');

    // Save analysis to database if we have an imageId
    if (imageId && pipelineResult.synthesizedResult) {
      try {
        console.log('üíæ Saving enhanced analysis to database...');
        const analysisDataWithMetadata = {
          ...pipelineResult.synthesizedResult,
          visionMetadata: pipelineResult.visionMetadata,
          analysisContext,
          enhancedPrompt: prompt
        };
        await saveAnalysisToDatabase(imageId, analysisDataWithMetadata);
        console.log('‚úÖ Enhanced analysis saved to database successfully');
      } catch (saveError) {
        console.error('‚ö†Ô∏è Failed to save enhanced analysis to database:', saveError);
        // Don't fail the request if database save fails
      }
    }

    return new Response(
      JSON.stringify({
        success: true,
        data: enhancedResult
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 200
      }
    );

  } catch (error) {
    console.error('‚ùå Enhanced context analysis failed:', error);
    
    return new Response(
      JSON.stringify({
        success: false,
        error: error.message,
        stage: 'enhanced-context-analysis',
        details: error.stack,
        timestamp: new Date().toISOString()
      }),
      { 
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    );
  }
}